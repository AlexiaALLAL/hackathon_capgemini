{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gOxKwriXdqz",
        "outputId": "4232cfe9-7710-458b-c5dd-ae214bbe2656"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import jaccard_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "# from utils.collate import pad_collate\n",
        "# from dataset import BaselineDataset\n",
        "# # from baseline.model import SimpleSegmentationModel\n",
        "# # from torchvision.models.vision_transformer import VisionTransformer\n",
        "# # from baseline.SegmentationViT import SegmentationViT\n",
        "# # from baseline.model_vision_transformer import TemporalVisionTransformer\n",
        "\n",
        "# from models import TemporalViTEncoder, ConvTransformerTokensToEmbeddingNeck, ViTConvNeckModel"
      ],
      "metadata": {
        "id": "pqqUxZOeV17Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "ME1HSGZxV8Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections.abc\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def pad_tensor(x, l, pad_value=0):\n",
        "    padlen = l - x.shape[0]\n",
        "    pad = [0 for _ in range(2 * len(x.shape[1:]))] + [0, padlen]\n",
        "    return F.pad(x, pad=pad, value=pad_value)\n",
        "\n",
        "\n",
        "np_str_obj_array_pattern = re.compile(r\"[SaUO]\")\n",
        "\n",
        "\n",
        "def pad_collate(batch, pad_value=0):\n",
        "    # Utility function to be used as collate_fn for the PyTorch dataloader\n",
        "    # to handle sequences of varying length.\n",
        "    # Sequences are padded with zeros by default.\n",
        "    #\n",
        "    # Modified default_collate from the official pytorch repo\n",
        "    # https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if len(elem.shape) > 0:\n",
        "            sizes = [e.shape[0] for e in batch]\n",
        "            m = max(sizes)\n",
        "            if not all(s == m for s in sizes):\n",
        "                # pad tensors which have a temporal dimension\n",
        "                batch = [pad_tensor(e, m, pad_value=pad_value) for e in batch]\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.stack(batch, 0, out=out)\n",
        "    elif (\n",
        "        elem_type.__module__ == \"numpy\"\n",
        "        and elem_type.__name__ != \"str_\"\n",
        "        and elem_type.__name__ != \"string_\"\n",
        "    ):\n",
        "        if elem_type.__name__ == \"ndarray\" or elem_type.__name__ == \"memmap\":\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(\"Format not managed : {}\".format(elem.dtype))\n",
        "\n",
        "            return pad_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "\n",
        "    elif isinstance(elem, collections.abc.Mapping):\n",
        "        return {key: pad_collate([d[key] for d in batch]) for key in elem}\n",
        "\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, \"_fields\"):  # namedtuple\n",
        "        return elem_type(*(pad_collate(samples) for samples in zip(*batch)))\n",
        "\n",
        "    elif isinstance(elem, collections.abc.Sequence):\n",
        "        # check to make sure that the elements in batch have consistent size\n",
        "        it = iter(batch)\n",
        "        elem_size = len(next(it))\n",
        "        if not all(len(elem) == elem_size for elem in it):\n",
        "            raise RuntimeError(\"each element in list of batch should be of equal size\")\n",
        "        transposed = zip(*batch)\n",
        "        return [pad_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(\"Format not managed : {}\".format(elem_type))\n"
      ],
      "metadata": {
        "id": "gPqm4TGAV9iZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "LRuBluIAWEDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class BaselineDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder: Path, channels = [2,3,4,5,8,9]):\n",
        "        super(BaselineDataset, self).__init__()\n",
        "        self.folder = folder\n",
        "\n",
        "        # Get metadata\n",
        "        print(\"Reading patch metadata ...\")\n",
        "        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n",
        "        self.meta_patch.index = self.meta_patch[\"ID\"].astype(int)\n",
        "        self.meta_patch.sort_index(inplace=True)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        self.len = self.meta_patch.shape[0]\n",
        "        self.id_patches = self.meta_patch.index\n",
        "        print(\"Dataset ready.\")\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item: int) -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n",
        "        id_patch = self.id_patches[item]\n",
        "\n",
        "        # Open and prepare satellite data into T x C x H x W arrays\n",
        "        path_patch = os.path.join(self.folder, \"DATA_S2\", \"S2_{}.npy\".format(id_patch))\n",
        "        data = np.load(path_patch).astype(np.float32)\n",
        "        data = torch.from_numpy(data)[:,self.channels,:,:]\n",
        "\n",
        "        # Open and prepare targets\n",
        "        target = np.load(\n",
        "            os.path.join(self.folder, \"ANNOTATIONS\", \"TARGET_{}.npy\".format(id_patch))\n",
        "        )\n",
        "        target = torch.from_numpy(target[0].astype(int))\n",
        "\n",
        "        return data, target\n",
        "\n",
        "class BaselineDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder: Path, channels = [2,3,4,5,8,9]):\n",
        "        super(BaselineDatasetTest, self).__init__()\n",
        "        self.folder = folder\n",
        "\n",
        "        # Get metadata\n",
        "        print(\"Reading patch metadata ...\")\n",
        "        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n",
        "        self.meta_patch.index = self.meta_patch[\"ID\"].astype(int)\n",
        "        self.meta_patch.sort_index(inplace=True)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        self.len = self.meta_patch.shape[0]\n",
        "        self.id_patches = self.meta_patch.index\n",
        "        print(\"Dataset ready.\")\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item: int) -> dict[str, torch.Tensor]:\n",
        "        id_patch = self.id_patches[item]\n",
        "\n",
        "        # Open and prepare satellite data into T x C x H x W arrays\n",
        "        path_patch = os.path.join(self.folder, \"DATA_S2\", \"S2_{}.npy\".format(id_patch))\n",
        "        data = np.load(path_patch).astype(np.float32)\n",
        "        data = torch.from_numpy(data)[:,self.channels,:,:]\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "0eS0Ay_QWFL-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "Nc0djQ_VWNVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "K_UNvk4hWTlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5c876b-5c05-407c-991c-e5f60ff951b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# --------------------------------------------------------\n",
        "# References:\n",
        "# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
        "# DeiT: https://github.com/facebookresearch/deit\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from timm.models.layers import to_2tuple\n",
        "from timm.models.vision_transformer import Block\n",
        "from typing import List\n",
        "import torchvision.transforms as vt\n",
        "\n",
        "def _convTranspose2dOutput(\n",
        "    input_size: int,\n",
        "    stride: int,\n",
        "    padding: int,\n",
        "    dilation: int,\n",
        "    kernel_size: int,\n",
        "    output_padding: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate the output size of a ConvTranspose2d.\n",
        "    Taken from: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
        "    \"\"\"\n",
        "    return (\n",
        "        (input_size - 1) * stride\n",
        "        - 2 * padding\n",
        "        + dilation * (kernel_size - 1)\n",
        "        + output_padding\n",
        "        + 1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: torch.Tensor):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
        "    omega /= embed_dim / 2.0\n",
        "    omega = 1.0 / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_3d_sincos_pos_embed(embed_dim: int, grid_size: tuple, cls_token: bool = False):\n",
        "    # Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "    # All rights reserved.\n",
        "\n",
        "    # This source code is licensed under the license found in the\n",
        "    # LICENSE file in the root directory of this source tree.\n",
        "    # --------------------------------------------------------\n",
        "    # Position embedding utils\n",
        "    # --------------------------------------------------------\n",
        "    \"\"\"\n",
        "    grid_size: 3d tuple of grid size: t, h, w\n",
        "    return:\n",
        "    pos_embed: L, D\n",
        "    \"\"\"\n",
        "\n",
        "    assert embed_dim % 16 == 0\n",
        "\n",
        "    t_size, h_size, w_size = grid_size\n",
        "\n",
        "    w_embed_dim = embed_dim // 16 * 6\n",
        "    h_embed_dim = embed_dim // 16 * 6\n",
        "    t_embed_dim = embed_dim // 16 * 4\n",
        "\n",
        "    w_pos_embed = get_1d_sincos_pos_embed_from_grid(w_embed_dim, np.arange(w_size))\n",
        "    h_pos_embed = get_1d_sincos_pos_embed_from_grid(h_embed_dim, np.arange(h_size))\n",
        "    t_pos_embed = get_1d_sincos_pos_embed_from_grid(t_embed_dim, np.arange(t_size))\n",
        "\n",
        "    w_pos_embed = np.tile(w_pos_embed, (t_size * h_size, 1))\n",
        "    h_pos_embed = np.tile(np.repeat(h_pos_embed, w_size, axis=0), (t_size, 1))\n",
        "    t_pos_embed = np.repeat(t_pos_embed, h_size * w_size, axis=0)\n",
        "\n",
        "    pos_embed = np.concatenate((w_pos_embed, h_pos_embed, t_pos_embed), axis=1)\n",
        "\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Frames of 2D Images to Patch Embedding\n",
        "    The 3D version of timm.models.vision_transformer.PatchEmbed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 224,\n",
        "        patch_size: int = 16,\n",
        "        num_frames: int = 3,\n",
        "        tubelet_size: int = 1,\n",
        "        in_chans: int = 3,\n",
        "        embed_dim: int = 768,\n",
        "        norm_layer: nn.Module = None,\n",
        "        flatten: bool = True,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_frames = num_frames\n",
        "        self.tubelet_size = tubelet_size\n",
        "        self.grid_size = (\n",
        "            num_frames // tubelet_size,\n",
        "            img_size[0] // patch_size[0],\n",
        "            img_size[1] // patch_size[1],\n",
        "        )\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]\n",
        "        self.flatten = flatten\n",
        "\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=(tubelet_size, patch_size[0], patch_size[1]),\n",
        "            stride=(tubelet_size, patch_size[0], patch_size[1]),\n",
        "            bias=bias,\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, T, H, W = x.shape\n",
        "        assert (\n",
        "            H == self.img_size[0]\n",
        "        ), f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\"\n",
        "        assert (\n",
        "            W == self.img_size[1]\n",
        "        ), f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\"\n",
        "        x = self.proj(x)\n",
        "        Hp, Wp = x.shape[3], x.shape[4]\n",
        "        if self.flatten:\n",
        "            x = x.flatten(2).transpose(1, 2)  # B,C,T,H,W -> B,C,L -> B,L,C\n",
        "        x = self.norm(x)\n",
        "        return x, Hp, Wp\n",
        "\n",
        "\n",
        "class Norm2d(nn.Module):\n",
        "    def __init__(self, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = self.ln(x)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTransformerTokensToEmbeddingNeck(nn.Module):\n",
        "    \"\"\"\n",
        "    Neck that transforms the token-based output of transformer into a single embedding suitable for processing with standard layers.\n",
        "    Performs 4 ConvTranspose2d operations on the rearranged input with kernel_size=2 and stride=2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        output_embed_dim: int,\n",
        "        # num_frames: int = 1,\n",
        "        Hp: int = 14,\n",
        "        Wp: int = 14,\n",
        "        drop_cls_token: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            embed_dim (int): Input embedding dimension\n",
        "            output_embed_dim (int): Output embedding dimension\n",
        "            Hp (int, optional): Height (in patches) of embedding to be upscaled. Defaults to 14.\n",
        "            Wp (int, optional): Width (in patches) of embedding to be upscaled. Defaults to 14.\n",
        "            drop_cls_token (bool, optional): Whether there is a cls_token, which should be dropped. This assumes the cls token is the first token. Defaults to True.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.drop_cls_token = drop_cls_token\n",
        "        self.Hp = Hp\n",
        "        self.Wp = Wp\n",
        "        self.H_out = Hp\n",
        "        self.W_out = Wp\n",
        "        # self.num_frames = num_frames\n",
        "\n",
        "        kernel_size = 2\n",
        "        stride = 2\n",
        "        dilation = 1\n",
        "        padding = 0\n",
        "        output_padding = 0\n",
        "        for _ in range(4):\n",
        "            self.H_out = _convTranspose2dOutput(\n",
        "                self.H_out, stride, padding, dilation, kernel_size, output_padding\n",
        "            )\n",
        "            self.W_out = _convTranspose2dOutput(\n",
        "                self.W_out, stride, padding, dilation, kernel_size, output_padding\n",
        "            )\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.output_embed_dim = output_embed_dim\n",
        "        self.fpn1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                self.embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "            Norm2d(self.output_embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                self.output_embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "        )\n",
        "        self.fpn2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                self.output_embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "            Norm2d(self.output_embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                self.output_embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_cls_token:\n",
        "            x = x[:, 1:, :]\n",
        "        x = x.permute(0, 2, 1).reshape(x.shape[0], -1, self.Hp, self.Wp)\n",
        "\n",
        "        x = self.fpn1(x)\n",
        "        x = self.fpn2(x)\n",
        "\n",
        "        x = x.reshape((-1, self.output_embed_dim, self.H_out, self.W_out))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TemporalViTEncoder(nn.Module):\n",
        "    \"\"\"Encoder from an ViT with capability to take in temporal input.\n",
        "\n",
        "    This class defines an encoder taken from a ViT architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 224,\n",
        "        patch_size: int = 16,\n",
        "        num_frames: int = 1,\n",
        "        tubelet_size: int = 1,\n",
        "        in_chans: int = 3,\n",
        "        embed_dim: int = 1024,\n",
        "        depth: int = 24,\n",
        "        num_heads: int = 16,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        norm_layer: nn.Module = nn.LayerNorm,\n",
        "        norm_pix_loss: bool = False,\n",
        "        pretrained: str = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            img_size (int, optional): Input image size. Defaults to 224.\n",
        "            patch_size (int, optional): Patch size to be used by the transformer. Defaults to 16.\n",
        "            num_frames (int, optional): Number of frames (temporal dimension) to be input to the encoder. Defaults to 1.\n",
        "            tubelet_size (int, optional): Tubelet size used in patch embedding. Defaults to 1.\n",
        "            in_chans (int, optional): Number of input channels. Defaults to 3.\n",
        "            embed_dim (int, optional): Embedding dimension. Defaults to 1024.\n",
        "            depth (int, optional): Encoder depth. Defaults to 24.\n",
        "            num_heads (int, optional): Number of heads used in the encoder blocks. Defaults to 16.\n",
        "            mlp_ratio (float, optional): Ratio to be used for the size of the MLP in encoder blocks. Defaults to 4.0.\n",
        "            norm_layer (nn.Module, optional): Norm layer to be used. Defaults to nn.LayerNorm.\n",
        "            norm_pix_loss (bool, optional): Whether to use Norm Pix Loss. Defaults to False.\n",
        "            pretrained (str, optional): Path to pretrained encoder weights. Defaults to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size, patch_size, num_frames, tubelet_size, in_chans, embed_dim\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False\n",
        "        )  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    mlp_ratio,\n",
        "                    qkv_bias=True,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "        self.pretrained = pretrained\n",
        "\n",
        "    #     self.initialize_weights()\n",
        "\n",
        "    # def initialize_weights(self):\n",
        "    #     # initialization\n",
        "    #     # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "    #     pos_embed = get_3d_sincos_pos_embed(\n",
        "    #         self.pos_embed.shape[-1], self.patch_embed.grid_size, cls_token=True\n",
        "    #     )\n",
        "    #     self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "    #     # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "    #     w = self.patch_embed.proj.weight.data\n",
        "    #     torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "    #     if isinstance(self.pretrained, str):\n",
        "    #         self.apply(self._init_weights)\n",
        "    #         print(f\"load from {self.pretrained}\")\n",
        "    #         load_checkpoint(self, self.pretrained, strict=False, map_location=\"cpu\")\n",
        "    #     elif self.pretrained is None:\n",
        "    #         # # initialize nn.Linear and nn.LayerNorm\n",
        "    #         self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # embed patches\n",
        "        x, _, _ = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTConvNeckModel(nn.Module):\n",
        "    def __init__(self, vit_encoder, conv_neck, num_classes):\n",
        "        super(ViTConvNeckModel, self).__init__()\n",
        "        self.vit_encoder = vit_encoder\n",
        "        self.conv_neck = conv_neck\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(self.conv_neck.output_embed_dim, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1),  # Final layer without padding\n",
        "            # nn.Softmax(dim=1)  # Apply softmax across the channel dimension\n",
        "        )\n",
        "\n",
        "        self.encoder_im_size = self.vit_encoder.patch_embed.img_size[0]\n",
        "        self.encoder_temp_length = self.vit_encoder.num_frames\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [B, T, C, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.permute(0,2,3,4,1) #[B, T, C, H, W] -> [B, C, H, W, T]\n",
        "\n",
        "        # Interpolate to make T = 3 and resize H and W for encoder\n",
        "        x_resized = F.interpolate(x, size=(H, W, self.encoder_temp_length))\n",
        "        x = x_resized.permute(0,1,4,2,3) #[B, C, H, W, T] -> [B, C, T, H, W]\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        # Define padding values\n",
        "        pad_h = (self.encoder_im_size - H) // 2\n",
        "        pad_w = (self.encoder_im_size - W) // 2\n",
        "\n",
        "        # Define the transform with padding mode 'edge'\n",
        "        pad_transform = vt.Pad(padding=(pad_w, pad_h), padding_mode='edge')\n",
        "        x_reshaped = x.reshape(-1, T, H, W)\n",
        "        x_padded = pad_transform(x_reshaped)\n",
        "        x_padded = x_padded.view(B, C, T, self.encoder_im_size, self.encoder_im_size)\n",
        "\n",
        "        # Pass through ViT encoder\n",
        "        vit_output = self.vit_encoder(x_padded)\n",
        "\n",
        "        # Pass through the convolutional neck to transform tokens into spatial embeddings\n",
        "        neck_output = self.conv_neck(vit_output)\n",
        "\n",
        "        # Output shape: [B, num_classes, H_out, W_out], apply bilinear upsampling to match input size\n",
        "        # resized_output = F.interpolate(neck_output, size=(H, W), mode='bilinear', align_corners=True)\n",
        "        H_out, W_out = neck_output.shape[-2:]\n",
        "        crop_h_start = (H_out - H) // 2\n",
        "        crop_w_start = (W_out - W) // 2\n",
        "        neck_output_cropped = neck_output[:, :, crop_h_start:crop_h_start + H, crop_w_start:crop_w_start + W]\n",
        "\n",
        "        output_final = self.head(neck_output_cropped)\n",
        "\n",
        "        return output_final\n"
      ],
      "metadata": {
        "id": "21E--4U4WMzY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "h3iyq4TQWfsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = Path(\"/content/drive/My Drive/\")  # Path to your Google Drive\n",
        "DATA_PATH_TRAIN = DIR / \"data-challenge-invent-mines-2024/DATA/DATA/TRAIN\"  # Replace 'dataset' with the actual folder name where your data is stored\n",
        "CHECKPOINT_PATH = DIR / \"multi_temporal_crop_classification_Prithvi_100M.pth\"\n",
        "LAST_CHECKPOINT_PATH = DIR / \"checkpoints/vit_epoch1.pth\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5m5qtD-vqn",
        "outputId": "a473634d-a598-40f2-9832-b8f804a7d06b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_iou_per_class(\n",
        "    targets: torch.Tensor,\n",
        "    preds: torch.Tensor,\n",
        "    nb_classes: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compute IoU between predictions and targets, for each class.\n",
        "\n",
        "    Args:\n",
        "        targets (torch.Tensor): Ground truth of shape (B, H, W).\n",
        "        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n",
        "        nb_classes (int): Number of classes in the segmentation task.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute IoU for each class\n",
        "    # Note: I use this for loop to iterate also on classes not in the demo batch\n",
        "\n",
        "    iou_per_class = []\n",
        "    for class_id in range(nb_classes):\n",
        "        iou = jaccard_score(\n",
        "            targets == class_id,\n",
        "            preds == class_id,\n",
        "            average=\"binary\",\n",
        "            zero_division=0,\n",
        "        )\n",
        "        iou_per_class.append(iou)\n",
        "\n",
        "    for class_id, iou in enumerate(iou_per_class):\n",
        "        print(\n",
        "            \"class {} - IoU: {:.4f} - targets: {} - preds: {}\".format(\n",
        "                class_id, iou, (targets == class_id).sum(), (preds == class_id).sum()\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "def print_mean_iou(targets: torch.Tensor, preds: torch.Tensor) -> None:\n",
        "    \"\"\"\n",
        "    Compute mean IoU between predictions and targets.\n",
        "\n",
        "    Args:\n",
        "        targets (torch.Tensor): Ground truth of shape (B, H, W).\n",
        "        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n",
        "    \"\"\"\n",
        "\n",
        "    mean_iou = jaccard_score(targets, preds, average=\"macro\")\n",
        "    print(f\"meanIOU (over existing classes in targets): {mean_iou:.4f}\")\n",
        "\n",
        "def split_state_dict(state_dict: OrderedDict) -> dict:\n",
        "    # Create dictionaries for each component\n",
        "    backbone_dict = OrderedDict()\n",
        "    neck_dict = OrderedDict()\n",
        "    decode_head_dict = OrderedDict()\n",
        "    auxiliary_head_dict = OrderedDict()\n",
        "\n",
        "    # Iterate through the state_dict and classify based on the prefix\n",
        "    for key, value in state_dict.items():\n",
        "        if key.startswith(\"backbone.\"):\n",
        "            backbone_dict[key[len(\"backbone.\"):]] = value  # Remove the prefix for cleaner dict\n",
        "        elif key.startswith(\"neck.\"):\n",
        "            neck_dict[key[len(\"neck.\"):]] = value\n",
        "        elif key.startswith(\"decode_head.\"):\n",
        "            decode_head_dict[key[len(\"decode_head.\"):]] = value\n",
        "        elif key.startswith(\"auxiliary_head.\"):\n",
        "            auxiliary_head_dict[key[len(\"auxiliary_head.\"):]] = value\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone_dict,\n",
        "        \"neck\": neck_dict,\n",
        "        \"decode_head\": decode_head_dict,\n",
        "        \"auxiliary_head\": auxiliary_head_dict\n",
        "    }\n",
        "\n",
        "def custom_model_init(checkpoint='model/multi_temporal_crop_classification_Prithvi_100M.pth', device = \"cpu\"):\n",
        "    state_dict = torch.load(checkpoint, map_location=torch.device(device))['state_dict']\n",
        "    split_dicts = split_state_dict(state_dict)\n",
        "\n",
        "    backbone_state_dict = split_dicts[\"backbone\"]\n",
        "    neck_state_dict = split_dicts[\"neck\"]\n",
        "    decode_head_state_dict = split_dicts[\"decode_head\"]\n",
        "    auxiliary_head_state_dict = split_dicts[\"auxiliary_head\"]\n",
        "\n",
        "    # Params\n",
        "    num_frames = 3\n",
        "    img_size = 224\n",
        "    num_workers = 2\n",
        "\n",
        "    num_layers = 6\n",
        "    patch_size = 16\n",
        "    embed_dim = 768\n",
        "    num_heads = 8\n",
        "    tubelet_size = 1\n",
        "    max_epochs = 80\n",
        "    eval_epoch_interval = 5\n",
        "\n",
        "    bands = [0,1,2,3,4,5]\n",
        "    output_embed_dim = embed_dim * num_frames\n",
        "\n",
        "    # You can now use these to load the corresponding nn.Module parts\n",
        "    vit_encoder = TemporalViTEncoder(\n",
        "        img_size=img_size,\n",
        "        patch_size=patch_size,\n",
        "        num_frames=num_frames,\n",
        "        tubelet_size=tubelet_size,\n",
        "        in_chans=len(bands),\n",
        "        embed_dim=768,\n",
        "        depth=6,\n",
        "        num_heads=num_heads,\n",
        "        mlp_ratio=4.0,\n",
        "        norm_pix_loss=False\n",
        "    )\n",
        "    missing_keys, unexpected_keys = vit_encoder.load_state_dict(backbone_state_dict, strict=False)\n",
        "    if missing_keys or unexpected_keys:\n",
        "        print(\"VIT Encoder INIT:\")\n",
        "        print(f\"Missing keys: {missing_keys}\")\n",
        "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
        "\n",
        "    conv_neck = ConvTransformerTokensToEmbeddingNeck(\n",
        "        embed_dim=embed_dim * num_frames,\n",
        "        output_embed_dim=output_embed_dim,\n",
        "        drop_cls_token=True,\n",
        "        Hp=14,\n",
        "        Wp=14,\n",
        "    )\n",
        "    missing_keys, unexpected_keys = conv_neck.load_state_dict(neck_state_dict, strict=False)\n",
        "    if missing_keys or unexpected_keys:\n",
        "        print(\"ConvNeck Encoder INIT:\")\n",
        "        print(f\"Missing keys: {missing_keys}\")\n",
        "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
        "\n",
        "    # ViTConvNeckModel(\n",
        "    #     vit_encoder=vit_encoder,\n",
        "    #     conv_neck=conv_neck,\n",
        "    # )\n",
        "\n",
        "    # Initialize the adaptive model with 10 classes (or any other number of classes)\n",
        "    return ViTConvNeckModel(\n",
        "        vit_encoder=vit_encoder,\n",
        "        conv_neck=conv_neck,\n",
        "        num_classes=20\n",
        "    )\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    data_folder: Path,\n",
        "    nb_classes: int,\n",
        "    input_channels: int,\n",
        "    num_epochs: int = 10,\n",
        "    accumulation_steps = 10,\n",
        "    batch_size: int = 4,\n",
        "    learning_rate: float = 1e-3,\n",
        "    device: str = \"cpu\",\n",
        "    verbose: bool = False,\n",
        ") -> ViTConvNeckModel:\n",
        "    \"\"\"\n",
        "    Training pipeline.\n",
        "    \"\"\"\n",
        "    # Create data loader\n",
        "    dataset = BaselineDataset(data_folder)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    # model = SimpleSegmentationModel(input_channels, nb_classes)\n",
        "    # model = custom_model_init(checkpoint=CHECKPOINT_PATH)\n",
        "    model = torch.load(LAST_CHECKPOINT_PATH, map_location=torch.device('cpu'))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Move the model to the appropriate device (GPU if available)\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        concat_target = torch.Tensor().to(device)\n",
        "        concat_preds = torch.Tensor().to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device)  # Satellite data\n",
        "            targets = targets.long()\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Normalize the loss by the number of accumulation steps\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "            # Backward pass (accumulate gradients)\n",
        "            loss.backward()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "            # Only perform optimizer step every accumulation_steps batches\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
        "                # Perform optimizer step\n",
        "                optimizer.step()\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Get the predicted class per pixel (B, H, W)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Move data from GPU/Metal to CPU\n",
        "            concat_target = torch.cat([concat_target, targets.view(-1)], dim=0)\n",
        "            concat_preds = torch.cat([concat_preds, preds.view(-1)], dim=0)\n",
        "\n",
        "            if verbose and (i + 1) % accumulation_steps == 0:\n",
        "                # Print mean IoU for debugging after every 10 batches\n",
        "                print_mean_iou(concat_target.cpu().numpy(), concat_preds.cpu().numpy())\n",
        "                concat_target = torch.Tensor().to(device)\n",
        "                concat_preds = torch.Tensor().to(device)\n",
        "\n",
        "        # Print the loss for this epoch\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "        torch.save(model, DIR / f\"checkpoints/vit_prithvi{num_epochs}.pth\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "sKJz_kqpWfXW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(\n",
        "    data_folder=Path(DATA_PATH_TRAIN),\n",
        "    nb_classes=20,\n",
        "    input_channels=6,\n",
        "    num_epochs=2,\n",
        "    batch_size=4,\n",
        "    accumulation_steps = 32,\n",
        "    learning_rate=1e-4,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "QG0mwB6iWlsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffbd9ae-e07f-45ff-d4bf-992f9372bdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading patch metadata ...\n",
            "Done.\n",
            "Dataset ready.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-b50d3ba73bdc>:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(LAST_CHECKPOINT_PATH, map_location=torch.device('cpu'))\n",
            "  7%|▋         | 32/490 [04:58<1:10:35,  9.25s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meanIOU (over existing classes in targets): 0.0211\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 64/490 [09:50<1:05:02,  9.16s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meanIOU (over existing classes in targets): 0.0192\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|█▉        | 96/490 [14:41<1:00:10,  9.16s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meanIOU (over existing classes in targets): 0.0187\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 128/490 [19:37<56:32,  9.37s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meanIOU (over existing classes in targets): 0.0203\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 160/490 [24:26<50:19,  9.15s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meanIOU (over existing classes in targets): 0.0185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 192/490 [29:19<46:37,  9.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meanIOU (over existing classes in targets): 0.0214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 224/490 [34:12<41:02,  9.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meanIOU (over existing classes in targets): 0.0023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 231/490 [35:17<40:45,  9.44s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST"
      ],
      "metadata": {
        "id": "o4_XQ0YC8Nr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH_TEST = DIR / \"data-challenge-invent-mines-2024/DATA/DATA/TEST\"  # Replace 'dataset' with the actual folder name where your data is stored\n",
        "CHECKPOINT_PATH = DIR / \"multi_temporal_crop_classification_Prithvi_100M.pth\"\n",
        "LAST_CHECKPOINT_PATH = DIR / \"checkpoints/vit_epoch1.pth\""
      ],
      "metadata": {
        "id": "4SYdNDDE9az6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def masks_to_str(predictions: np.ndarray) -> list[str]:\n",
        "    \"\"\"\n",
        "    Convert the\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray): predictions as a 3D batch (B, H, W)\n",
        "\n",
        "    Returns:\n",
        "        list[str]: a list of B strings, each string is a flattened stringified prediction mask\n",
        "    \"\"\"\n",
        "    return [\" \".join(f\"{x}\" for x in np.ravel(x)) for x in predictions]\n",
        "\n",
        "\n",
        "def decode_masks(\n",
        "    masks: list[str],\n",
        "    target_shape: tuple[int, int] = (128, 128),\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert each string in masks back to a 1D list of integers.\n",
        "\n",
        "    Args:\n",
        "        masks (list[str]): list of stringified masks\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: reconstructed batch of masks\n",
        "    \"\"\"\n",
        "    return np.array(\n",
        "        [\n",
        "            np.fromstring(mask, sep=\" \", dtype=np.uint8).reshape(target_shape)\n",
        "            for mask in masks\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def test_model(\n",
        "        name: str,\n",
        "        checkpoint_path: str,\n",
        "        input_channels: int,\n",
        "        nb_classes: int,\n",
        "        data_folder: Path,\n",
        "        batch_size: int = 1,\n",
        "):\n",
        "    # Load model\n",
        "    # Initialize the model architecture\n",
        "    model = custom_model_init(checkpoint=CHECKPOINT_PATH)  # Skip loading the checkpoint here\n",
        "\n",
        "    # Load the saved state_dict\n",
        "    model = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = BaselineDatasetTest(data_folder)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    all_preds = torch.zeros(len(dataloader), 128, 128)\n",
        "    for i, images in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        with torch.no_grad():\n",
        "            preds = model(images)  # Only the 10th image\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "        all_preds[batch_size*i:batch_size*(i+1)] = preds\n",
        "\n",
        "    all_preds = all_preds.int()\n",
        "\n",
        "    # all_preds_flat = all_preds.cpu().numpy().flatten()\n",
        "\n",
        "    # Print mIoU for the test set\n",
        "    # print_iou_per_class(all_targets_flat, all_preds_flat, nb_classes)\n",
        "    # print_mean_iou(all_targets_flat, all_preds_flat)\n",
        "\n",
        "    # Generate the csv submission file\n",
        "    masks = masks_to_str(all_preds)\n",
        "    submission = pd.DataFrame.from_dict({\"ID\": range(len(all_preds)), \"MASKS\": masks})\n",
        "    submission[\"ID\"] = submission[\"ID\"] + 20000\n",
        "    submission.to_csv(DIR / f\"submissions/submission_{name}.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "_syeHtBD8M9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(\n",
        "    name=\"1epoch\",\n",
        "    checkpoint_path=LAST_CHECKPOINT_PATH,\n",
        "    input_channels=6,\n",
        "    nb_classes=20,\n",
        "    data_folder=Path(DATA_PATH_TEST),\n",
        "    batch_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "upV0y_MW9MpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}