{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gOxKwriXdqz",
        "outputId": "4232cfe9-7710-458b-c5dd-ae214bbe2656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pqqUxZOeV17Z"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import jaccard_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME1HSGZxV8Az"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gPqm4TGAV9iZ"
      },
      "outputs": [],
      "source": [
        "import collections.abc\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def pad_tensor(x, l, pad_value=0):\n",
        "    padlen = l - x.shape[0]\n",
        "    pad = [0 for _ in range(2 * len(x.shape[1:]))] + [0, padlen]\n",
        "    return F.pad(x, pad=pad, value=pad_value)\n",
        "\n",
        "\n",
        "np_str_obj_array_pattern = re.compile(r\"[SaUO]\")\n",
        "\n",
        "\n",
        "def pad_collate(batch, pad_value=0):\n",
        "    # Utility function to be used as collate_fn for the PyTorch dataloader\n",
        "    # to handle sequences of varying length.\n",
        "    # Sequences are padded with zeros by default.\n",
        "    #\n",
        "    # Modified default_collate from the official pytorch repo\n",
        "    # https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if len(elem.shape) > 0:\n",
        "            sizes = [e.shape[0] for e in batch]\n",
        "            m = max(sizes)\n",
        "            if not all(s == m for s in sizes):\n",
        "                # pad tensors which have a temporal dimension\n",
        "                batch = [pad_tensor(e, m, pad_value=pad_value) for e in batch]\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.stack(batch, 0, out=out)\n",
        "    elif (\n",
        "        elem_type.__module__ == \"numpy\"\n",
        "        and elem_type.__name__ != \"str_\"\n",
        "        and elem_type.__name__ != \"string_\"\n",
        "    ):\n",
        "        if elem_type.__name__ == \"ndarray\" or elem_type.__name__ == \"memmap\":\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(\"Format not managed : {}\".format(elem.dtype))\n",
        "\n",
        "            return pad_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "\n",
        "    elif isinstance(elem, collections.abc.Mapping):\n",
        "        return {key: pad_collate([d[key] for d in batch]) for key in elem}\n",
        "\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, \"_fields\"):  # namedtuple\n",
        "        return elem_type(*(pad_collate(samples) for samples in zip(*batch)))\n",
        "\n",
        "    elif isinstance(elem, collections.abc.Sequence):\n",
        "        # check to make sure that the elements in batch have consistent size\n",
        "        it = iter(batch)\n",
        "        elem_size = len(next(it))\n",
        "        if not all(len(elem) == elem_size for elem in it):\n",
        "            raise RuntimeError(\"each element in list of batch should be of equal size\")\n",
        "        transposed = zip(*batch)\n",
        "        return [pad_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(\"Format not managed : {}\".format(elem_type))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRuBluIAWEDW"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0eS0Ay_QWFL-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class BaselineDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder: Path, channels = [2,3,4,5,8,9]):\n",
        "        super(BaselineDataset, self).__init__()\n",
        "        self.folder = folder\n",
        "\n",
        "        # Get metadata\n",
        "        print(\"Reading patch metadata ...\")\n",
        "        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n",
        "        self.meta_patch.index = self.meta_patch[\"ID\"].astype(int)\n",
        "        self.meta_patch.sort_index(inplace=True)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        self.len = self.meta_patch.shape[0]\n",
        "        self.id_patches = self.meta_patch.index\n",
        "        print(\"Dataset ready.\")\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item: int) -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n",
        "        id_patch = self.id_patches[item]\n",
        "\n",
        "        # Open and prepare satellite data into T x C x H x W arrays\n",
        "        path_patch = os.path.join(self.folder, \"DATA_S2\", \"S2_{}.npy\".format(id_patch))\n",
        "        data = np.load(path_patch).astype(np.float32)\n",
        "        data = torch.from_numpy(data)[:,self.channels,:,:]\n",
        "\n",
        "        # Open and prepare targets\n",
        "        target = np.load(\n",
        "            os.path.join(self.folder, \"ANNOTATIONS\", \"TARGET_{}.npy\".format(id_patch))\n",
        "        )\n",
        "        target = torch.from_numpy(target[0].astype(int))\n",
        "\n",
        "        return data, target\n",
        "\n",
        "class BaselineDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder: Path, channels = [2,3,4,5,8,9]):\n",
        "        super(BaselineDatasetTest, self).__init__()\n",
        "        self.folder = folder\n",
        "\n",
        "        # Get metadata\n",
        "        print(\"Reading patch metadata ...\")\n",
        "        self.meta_patch = gpd.read_file(os.path.join(folder, \"metadata.geojson\"))\n",
        "        self.meta_patch.index = self.meta_patch[\"ID\"].astype(int)\n",
        "        self.meta_patch.sort_index(inplace=True)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        self.len = self.meta_patch.shape[0]\n",
        "        self.id_patches = self.meta_patch.index\n",
        "        print(\"Dataset ready.\")\n",
        "\n",
        "        self.channels = channels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, item: int) -> dict[str, torch.Tensor]:\n",
        "        id_patch = self.id_patches[item]\n",
        "\n",
        "        # Open and prepare satellite data into T x C x H x W arrays\n",
        "        path_patch = os.path.join(self.folder, \"DATA_S2\", \"S2_{}.npy\".format(id_patch))\n",
        "        data = np.load(path_patch).astype(np.float32)\n",
        "        data = torch.from_numpy(data)[:,self.channels,:,:]\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc0djQ_VWNVf"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_UNvk4hWTlt",
        "outputId": "4c5c876b-5c05-407c-991c-e5f60ff951b1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mÉchec du démarrage du Kernel. \n",
            "\u001b[1;31mAttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?. \n",
            "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
          ]
        }
      ],
      "source": [
        "#!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "21E--4U4WMzY"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "# --------------------------------------------------------\n",
        "# References:\n",
        "# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
        "# DeiT: https://github.com/facebookresearch/deit\n",
        "# --------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from timm.models.layers import to_2tuple\n",
        "from timm.models.vision_transformer import Block\n",
        "from typing import List\n",
        "import torchvision.transforms as vt\n",
        "\n",
        "def _convTranspose2dOutput(\n",
        "    input_size: int,\n",
        "    stride: int,\n",
        "    padding: int,\n",
        "    dilation: int,\n",
        "    kernel_size: int,\n",
        "    output_padding: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate the output size of a ConvTranspose2d.\n",
        "    Taken from: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
        "    \"\"\"\n",
        "    return (\n",
        "        (input_size - 1) * stride\n",
        "        - 2 * padding\n",
        "        + dilation * (kernel_size - 1)\n",
        "        + output_padding\n",
        "        + 1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: torch.Tensor):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
        "    omega /= embed_dim / 2.0\n",
        "    omega = 1.0 / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_3d_sincos_pos_embed(embed_dim: int, grid_size: tuple, cls_token: bool = False):\n",
        "    # Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "    # All rights reserved.\n",
        "\n",
        "    # This source code is licensed under the license found in the\n",
        "    # LICENSE file in the root directory of this source tree.\n",
        "    # --------------------------------------------------------\n",
        "    # Position embedding utils\n",
        "    # --------------------------------------------------------\n",
        "    \"\"\"\n",
        "    grid_size: 3d tuple of grid size: t, h, w\n",
        "    return:\n",
        "    pos_embed: L, D\n",
        "    \"\"\"\n",
        "\n",
        "    assert embed_dim % 16 == 0\n",
        "\n",
        "    t_size, h_size, w_size = grid_size\n",
        "\n",
        "    w_embed_dim = embed_dim // 16 * 6\n",
        "    h_embed_dim = embed_dim // 16 * 6\n",
        "    t_embed_dim = embed_dim // 16 * 4\n",
        "\n",
        "    w_pos_embed = get_1d_sincos_pos_embed_from_grid(w_embed_dim, np.arange(w_size))\n",
        "    h_pos_embed = get_1d_sincos_pos_embed_from_grid(h_embed_dim, np.arange(h_size))\n",
        "    t_pos_embed = get_1d_sincos_pos_embed_from_grid(t_embed_dim, np.arange(t_size))\n",
        "\n",
        "    w_pos_embed = np.tile(w_pos_embed, (t_size * h_size, 1))\n",
        "    h_pos_embed = np.tile(np.repeat(h_pos_embed, w_size, axis=0), (t_size, 1))\n",
        "    t_pos_embed = np.repeat(t_pos_embed, h_size * w_size, axis=0)\n",
        "\n",
        "    pos_embed = np.concatenate((w_pos_embed, h_pos_embed, t_pos_embed), axis=1)\n",
        "\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Frames of 2D Images to Patch Embedding\n",
        "    The 3D version of timm.models.vision_transformer.PatchEmbed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 224,\n",
        "        patch_size: int = 16,\n",
        "        num_frames: int = 3,\n",
        "        tubelet_size: int = 1,\n",
        "        in_chans: int = 3,\n",
        "        embed_dim: int = 768,\n",
        "        norm_layer: nn.Module = None,\n",
        "        flatten: bool = True,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_frames = num_frames\n",
        "        self.tubelet_size = tubelet_size\n",
        "        self.grid_size = (\n",
        "            num_frames // tubelet_size,\n",
        "            img_size[0] // patch_size[0],\n",
        "            img_size[1] // patch_size[1],\n",
        "        )\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]\n",
        "        self.flatten = flatten\n",
        "\n",
        "        self.proj = nn.Conv3d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=(tubelet_size, patch_size[0], patch_size[1]),\n",
        "            stride=(tubelet_size, patch_size[0], patch_size[1]),\n",
        "            bias=bias,\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, T, H, W = x.shape\n",
        "        assert (\n",
        "            H == self.img_size[0]\n",
        "        ), f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\"\n",
        "        assert (\n",
        "            W == self.img_size[1]\n",
        "        ), f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\"\n",
        "        x = self.proj(x)\n",
        "        Hp, Wp = x.shape[3], x.shape[4]\n",
        "        if self.flatten:\n",
        "            x = x.flatten(2).transpose(1, 2)  # B,C,T,H,W -> B,C,L -> B,L,C\n",
        "        x = self.norm(x)\n",
        "        return x, Hp, Wp\n",
        "\n",
        "\n",
        "class Norm2d(nn.Module):\n",
        "    def __init__(self, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = self.ln(x)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvTransformerTokensToEmbeddingNeck(nn.Module):\n",
        "    \"\"\"\n",
        "    Neck that transforms the token-based output of transformer into a single embedding suitable for processing with standard layers.\n",
        "    Performs 4 ConvTranspose2d operations on the rearranged input with kernel_size=2 and stride=2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        output_embed_dim: int,\n",
        "        Hp: int = 14,\n",
        "        Wp: int = 14,\n",
        "        drop_cls_token: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            embed_dim (int): Input embedding dimension\n",
        "            output_embed_dim (int): Output embedding dimension\n",
        "            Hp (int, optional): Height (in patches) of embedding to be upscaled. Defaults to 14.\n",
        "            Wp (int, optional): Width (in patches) of embedding to be upscaled. Defaults to 14.\n",
        "            drop_cls_token (bool, optional): Whether there is a cls_token, which should be dropped. This assumes the cls token is the first token. Defaults to True.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.drop_cls_token = drop_cls_token\n",
        "        self.Hp = Hp\n",
        "        self.Wp = Wp\n",
        "        self.H_out = Hp\n",
        "        self.W_out = Wp\n",
        "        # self.num_frames = num_frames\n",
        "\n",
        "        kernel_size = 2\n",
        "        stride = 2\n",
        "        dilation = 1\n",
        "        padding = 0\n",
        "        output_padding = 0\n",
        "        for _ in range(4):\n",
        "            self.H_out = _convTranspose2dOutput(\n",
        "                self.H_out, stride, padding, dilation, kernel_size, output_padding\n",
        "            )\n",
        "            self.W_out = _convTranspose2dOutput(\n",
        "                self.W_out, stride, padding, dilation, kernel_size, output_padding\n",
        "            )\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.output_embed_dim = output_embed_dim\n",
        "        self.fpn1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                self.embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "            Norm2d(self.output_embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                self.output_embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "        )\n",
        "        self.fpn2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                self.output_embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "            Norm2d(self.output_embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                self.output_embed_dim,\n",
        "                self.output_embed_dim,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                dilation=dilation,\n",
        "                padding=padding,\n",
        "                output_padding=output_padding,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_cls_token:\n",
        "            x = x[:, 1:, :]\n",
        "        x = x.permute(0, 2, 1).reshape(x.shape[0], -1, self.Hp, self.Wp)\n",
        "\n",
        "        x = self.fpn1(x)\n",
        "        x = self.fpn2(x)\n",
        "\n",
        "        x = x.reshape((-1, self.output_embed_dim, self.H_out, self.W_out))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TemporalViTEncoder(nn.Module):\n",
        "    \"\"\"Encoder from an ViT with capability to take in temporal input.\n",
        "\n",
        "    This class defines an encoder taken from a ViT architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 224,\n",
        "        patch_size: int = 16,\n",
        "        num_frames: int = 1,\n",
        "        tubelet_size: int = 1,\n",
        "        in_chans: int = 3,\n",
        "        embed_dim: int = 1024,\n",
        "        depth: int = 24,\n",
        "        num_heads: int = 16,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        norm_layer: nn.Module = nn.LayerNorm,\n",
        "        norm_pix_loss: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            img_size (int, optional): Input image size. Defaults to 224.\n",
        "            patch_size (int, optional): Patch size to be used by the transformer. Defaults to 16.\n",
        "            num_frames (int, optional): Number of frames (temporal dimension) to be input to the encoder. Defaults to 1.\n",
        "            tubelet_size (int, optional): Tubelet size used in patch embedding. Defaults to 1.\n",
        "            in_chans (int, optional): Number of input channels. Defaults to 3.\n",
        "            embed_dim (int, optional): Embedding dimension. Defaults to 1024.\n",
        "            depth (int, optional): Encoder depth. Defaults to 24.\n",
        "            num_heads (int, optional): Number of heads used in the encoder blocks. Defaults to 16.\n",
        "            mlp_ratio (float, optional): Ratio to be used for the size of the MLP in encoder blocks. Defaults to 4.0.\n",
        "            norm_layer (nn.Module, optional): Norm layer to be used. Defaults to nn.LayerNorm.\n",
        "            norm_pix_loss (bool, optional): Whether to use Norm Pix Loss. Defaults to False.\n",
        "            pretrained (str, optional): Path to pretrained encoder weights. Defaults to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size, patch_size, num_frames, tubelet_size, in_chans, embed_dim\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False\n",
        "        )  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    mlp_ratio,\n",
        "                    qkv_bias=True,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # embed patches\n",
        "        x, _, _ = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTConvNeckModel(nn.Module):\n",
        "    def __init__(self, vit_encoder, conv_neck, num_classes):\n",
        "        super(ViTConvNeckModel, self).__init__()\n",
        "        self.vit_encoder = vit_encoder\n",
        "        self.conv_neck = conv_neck\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(self.conv_neck.output_embed_dim, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, num_classes, kernel_size=1),  # Final layer without padding\n",
        "            # nn.Softmax(dim=1)  # Apply softmax across the channel dimension\n",
        "        )\n",
        "\n",
        "        self.encoder_im_size = self.vit_encoder.patch_embed.img_size[0]\n",
        "        self.encoder_temp_length = self.vit_encoder.num_frames\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [B, T, C, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.permute(0,2,3,4,1) #[B, T, C, H, W] -> [B, C, H, W, T]\n",
        "\n",
        "        # Interpolate to make T = 3 and resize H and W for encoder\n",
        "        x_resized = F.interpolate(x, size=(H, W, self.encoder_temp_length))\n",
        "        x = x_resized.permute(0,1,4,2,3) #[B, C, H, W, T] -> [B, C, T, H, W]\n",
        "        B, C, T, H, W = x.shape\n",
        "\n",
        "        # Define padding values\n",
        "        pad_h = (self.encoder_im_size - H) // 2\n",
        "        pad_w = (self.encoder_im_size - W) // 2\n",
        "\n",
        "        # Define the transform with padding mode 'edge'\n",
        "        pad_transform = vt.Pad(padding=(pad_w, pad_h), padding_mode='edge')\n",
        "        x_reshaped = x.reshape(-1, T, H, W)\n",
        "        x_padded = pad_transform(x_reshaped)\n",
        "        x_padded = x_padded.view(B, C, T, self.encoder_im_size, self.encoder_im_size)\n",
        "\n",
        "        # Pass through ViT encoder\n",
        "        vit_output = self.vit_encoder(x_padded)\n",
        "\n",
        "        # Pass through the convolutional neck to transform tokens into spatial embeddings\n",
        "        neck_output = self.conv_neck(vit_output)\n",
        "\n",
        "        # Output shape: [B, num_classes, H_out, W_out], apply bilinear upsampling to match input size\n",
        "        # resized_output = F.interpolate(neck_output, size=(H, W), mode='bilinear', align_corners=True)\n",
        "        H_out, W_out = neck_output.shape[-2:]\n",
        "        crop_h_start = (H_out - H) // 2\n",
        "        crop_w_start = (W_out - W) // 2\n",
        "        neck_output_cropped = neck_output[:, :, crop_h_start:crop_h_start + H, crop_w_start:crop_w_start + W]\n",
        "\n",
        "        output_final = self.head(neck_output_cropped)\n",
        "\n",
        "        return output_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3iyq4TQWfsI"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5m5qtD-vqn",
        "outputId": "a473634d-a598-40f2-9832-b8f804a7d06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "DIR = Path(\"/content/drive/My Drive/\")  # Path to your Google Drive\n",
        "DATA_PATH_TRAIN = DIR / \"data-challenge-invent-mines-2024/DATA/DATA/TRAIN\"  # Replace 'dataset' with the actual folder name where your data is stored\n",
        "CHECKPOINT_PATH = DIR / \"multi_temporal_crop_classification_Prithvi_100M.pth\"\n",
        "LAST_CHECKPOINT_PATH = DIR / \"checkpoints/vit_epoch1.pth\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sKJz_kqpWfXW"
      },
      "outputs": [],
      "source": [
        "def print_iou_per_class(\n",
        "    targets: torch.Tensor,\n",
        "    preds: torch.Tensor,\n",
        "    nb_classes: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compute IoU between predictions and targets, for each class.\n",
        "\n",
        "    Args:\n",
        "        targets (torch.Tensor): Ground truth of shape (B, H, W).\n",
        "        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n",
        "        nb_classes (int): Number of classes in the segmentation task.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute IoU for each class\n",
        "    # Note: I use this for loop to iterate also on classes not in the demo batch\n",
        "\n",
        "    iou_per_class = []\n",
        "    for class_id in range(nb_classes):\n",
        "        iou = jaccard_score(\n",
        "            targets == class_id,\n",
        "            preds == class_id,\n",
        "            average=\"binary\",\n",
        "            zero_division=0,\n",
        "        )\n",
        "        iou_per_class.append(iou)\n",
        "\n",
        "    for class_id, iou in enumerate(iou_per_class):\n",
        "        print(\n",
        "            \"class {} - IoU: {:.4f} - targets: {} - preds: {}\".format(\n",
        "                class_id, iou, (targets == class_id).sum(), (preds == class_id).sum()\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "def print_mean_iou(targets: torch.Tensor, preds: torch.Tensor) -> None:\n",
        "    \"\"\"\n",
        "    Compute mean IoU between predictions and targets.\n",
        "\n",
        "    Args:\n",
        "        targets (torch.Tensor): Ground truth of shape (B, H, W).\n",
        "        preds (torch.Tensor): Model predictions of shape (B, nb_classes, H, W).\n",
        "    \"\"\"\n",
        "\n",
        "    mean_iou = jaccard_score(targets, preds, average=\"macro\")\n",
        "    print(f\"meanIOU (over existing classes in targets): {mean_iou:.4f}\")\n",
        "\n",
        "def split_state_dict(state_dict: OrderedDict) -> dict:\n",
        "    # Create dictionaries for each component\n",
        "    backbone_dict = OrderedDict()\n",
        "    neck_dict = OrderedDict()\n",
        "    decode_head_dict = OrderedDict()\n",
        "    auxiliary_head_dict = OrderedDict()\n",
        "\n",
        "    # Iterate through the state_dict and classify based on the prefix\n",
        "    for key, value in state_dict.items():\n",
        "        if key.startswith(\"backbone.\"):\n",
        "            backbone_dict[key[len(\"backbone.\"):]] = value  # Remove the prefix for cleaner dict\n",
        "        elif key.startswith(\"neck.\"):\n",
        "            neck_dict[key[len(\"neck.\"):]] = value\n",
        "        elif key.startswith(\"decode_head.\"):\n",
        "            decode_head_dict[key[len(\"decode_head.\"):]] = value\n",
        "        elif key.startswith(\"auxiliary_head.\"):\n",
        "            auxiliary_head_dict[key[len(\"auxiliary_head.\"):]] = value\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone_dict,\n",
        "        \"neck\": neck_dict,\n",
        "        \"decode_head\": decode_head_dict,\n",
        "        \"auxiliary_head\": auxiliary_head_dict\n",
        "    }\n",
        "\n",
        "def custom_model_init(checkpoint='multi_temporal_crop_classification_Prithvi_100M.pth', device = \"cpu\"):\n",
        "    state_dict = torch.load(checkpoint, map_location=torch.device(device))['state_dict']\n",
        "    split_dicts = split_state_dict(state_dict)\n",
        "\n",
        "    backbone_state_dict = split_dicts[\"backbone\"]\n",
        "    neck_state_dict = split_dicts[\"neck\"]\n",
        "    decode_head_state_dict = split_dicts[\"decode_head\"]\n",
        "    auxiliary_head_state_dict = split_dicts[\"auxiliary_head\"]\n",
        "\n",
        "    # Params\n",
        "    num_frames = 3\n",
        "    img_size = 224\n",
        "    num_workers = 2\n",
        "\n",
        "    num_layers = 6\n",
        "    patch_size = 16\n",
        "    embed_dim = 768\n",
        "    num_heads = 8\n",
        "    tubelet_size = 1\n",
        "    max_epochs = 80\n",
        "    eval_epoch_interval = 5\n",
        "\n",
        "    bands = [0,1,2,3,4,5]\n",
        "    output_embed_dim = embed_dim * num_frames\n",
        "\n",
        "    # You can now use these to load the corresponding nn.Module parts\n",
        "    vit_encoder = TemporalViTEncoder(\n",
        "        img_size=img_size,\n",
        "        patch_size=patch_size,\n",
        "        num_frames=num_frames,\n",
        "        tubelet_size=tubelet_size,\n",
        "        in_chans=len(bands),\n",
        "        embed_dim=768,\n",
        "        depth=6,\n",
        "        num_heads=num_heads,\n",
        "        mlp_ratio=4.0,\n",
        "        norm_pix_loss=False\n",
        "    )\n",
        "    missing_keys, unexpected_keys = vit_encoder.load_state_dict(backbone_state_dict, strict=False)\n",
        "    if missing_keys or unexpected_keys:\n",
        "        print(\"VIT Encoder INIT:\")\n",
        "        print(f\"Missing keys: {missing_keys}\")\n",
        "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
        "\n",
        "    conv_neck = ConvTransformerTokensToEmbeddingNeck(\n",
        "        embed_dim=embed_dim * num_frames,\n",
        "        output_embed_dim=output_embed_dim,\n",
        "        drop_cls_token=True,\n",
        "        Hp=14,\n",
        "        Wp=14,\n",
        "    )\n",
        "    missing_keys, unexpected_keys = conv_neck.load_state_dict(neck_state_dict, strict=False)\n",
        "    if missing_keys or unexpected_keys:\n",
        "        print(\"ConvNeck Encoder INIT:\")\n",
        "        print(f\"Missing keys: {missing_keys}\")\n",
        "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
        "\n",
        "    return ViTConvNeckModel(\n",
        "        vit_encoder=vit_encoder,\n",
        "        conv_neck=conv_neck,\n",
        "        num_classes=20\n",
        "    )\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    data_folder: Path,\n",
        "    nb_classes: int,\n",
        "    input_channels: int,\n",
        "    num_epochs: int = 10,\n",
        "    accumulation_steps = 10,\n",
        "    batch_size: int = 4,\n",
        "    learning_rate: float = 1e-3,\n",
        "    device: str = \"cpu\",\n",
        "    verbose: bool = False,\n",
        ") -> ViTConvNeckModel:\n",
        "    \"\"\"\n",
        "    Training pipeline.\n",
        "    \"\"\"\n",
        "    # Create data loader\n",
        "    dataset = BaselineDataset(data_folder)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Initialize the model, loss function, and optimizer\n",
        "    # model = SimpleSegmentationModel(input_channels, nb_classes)\n",
        "    # model = custom_model_init(checkpoint=CHECKPOINT_PATH)\n",
        "    model = torch.load(LAST_CHECKPOINT_PATH, map_location=torch.device('cpu'))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Move the model to the appropriate device (GPU if available)\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        concat_target = torch.Tensor().to(device)\n",
        "        concat_preds = torch.Tensor().to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "            # Move data to device\n",
        "            inputs = inputs.to(device)  # Satellite data\n",
        "            targets = targets.long()\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Normalize the loss by the number of accumulation steps\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "            # Backward pass (accumulate gradients)\n",
        "            loss.backward()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item() * accumulation_steps\n",
        "\n",
        "            # Only perform optimizer step every accumulation_steps batches\n",
        "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n",
        "                # Perform optimizer step\n",
        "                optimizer.step()\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Get the predicted class per pixel (B, H, W)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Move data from GPU/Metal to CPU\n",
        "            concat_target = torch.cat([concat_target, targets.view(-1)], dim=0)\n",
        "            concat_preds = torch.cat([concat_preds, preds.view(-1)], dim=0)\n",
        "\n",
        "            if verbose and (i + 1) % accumulation_steps == 0:\n",
        "                # Print mean IoU for debugging after every 10 batches\n",
        "                print_mean_iou(concat_target.cpu().numpy(), concat_preds.cpu().numpy())\n",
        "                concat_target = torch.Tensor().to(device)\n",
        "                concat_preds = torch.Tensor().to(device)\n",
        "\n",
        "        # Print the loss for this epoch\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "        torch.save(model, DIR / f\"checkpoints/vit_prithvi_{num_epochs}.pth\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG0mwB6iWlsV",
        "outputId": "8ffbd9ae-e07f-45ff-d4bf-992f9372bdda"
      },
      "outputs": [],
      "source": [
        "model = train_model(\n",
        "    data_folder=Path(DATA_PATH_TRAIN),\n",
        "    nb_classes=20,\n",
        "    input_channels=6,\n",
        "    num_epochs=2,\n",
        "    batch_size=4,\n",
        "    accumulation_steps = 32,\n",
        "    learning_rate=1e-4,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4_XQ0YC8Nr7"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SYdNDDE9az6"
      },
      "outputs": [],
      "source": [
        "DATA_PATH_TEST = DIR / \"data-challenge-invent-mines-2024/DATA/DATA/TEST\"  # Replace 'dataset' with the actual folder name where your data is stored\n",
        "CHECKPOINT_PATH = DIR / \"multi_temporal_crop_classification_Prithvi_100M.pth\"\n",
        "LAST_CHECKPOINT_PATH = DIR / \"checkpoints/vit_prithvi_5.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_syeHtBD8M9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def masks_to_str(predictions: np.ndarray) -> list[str]:\n",
        "    \"\"\"\n",
        "    Convert the\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray): predictions as a 3D batch (B, H, W)\n",
        "\n",
        "    Returns:\n",
        "        list[str]: a list of B strings, each string is a flattened stringified prediction mask\n",
        "    \"\"\"\n",
        "    return [\" \".join(f\"{x}\" for x in np.ravel(x)) for x in predictions]\n",
        "\n",
        "\n",
        "def decode_masks(\n",
        "    masks: list[str],\n",
        "    target_shape: tuple[int, int] = (128, 128),\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert each string in masks back to a 1D list of integers.\n",
        "\n",
        "    Args:\n",
        "        masks (list[str]): list of stringified masks\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: reconstructed batch of masks\n",
        "    \"\"\"\n",
        "    return np.array(\n",
        "        [\n",
        "            np.fromstring(mask, sep=\" \", dtype=np.uint8).reshape(target_shape)\n",
        "            for mask in masks\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def test_model(\n",
        "        name: str,\n",
        "        checkpoint_path: str,\n",
        "        input_channels: int,\n",
        "        nb_classes: int,\n",
        "        data_folder: Path,\n",
        "        batch_size: int = 1,\n",
        "):\n",
        "    # Load model\n",
        "    # Initialize the model architecture\n",
        "    model = custom_model_init(checkpoint=CHECKPOINT_PATH)  # Skip loading the checkpoint here\n",
        "\n",
        "    # Load the saved state_dict\n",
        "    model = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = BaselineDatasetTest(data_folder)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    all_preds = torch.zeros(len(dataloader), 128, 128)\n",
        "    for i, images in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        with torch.no_grad():\n",
        "            preds = model(images)  # Only the 10th image\n",
        "            preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "        all_preds[batch_size*i:batch_size*(i+1)] = preds\n",
        "\n",
        "    all_preds = all_preds.int()\n",
        "\n",
        "    # Generate the csv submission file\n",
        "    masks = masks_to_str(all_preds)\n",
        "    submission = pd.DataFrame.from_dict({\"ID\": range(len(all_preds)), \"MASKS\": masks})\n",
        "    submission[\"ID\"] = submission[\"ID\"] + 20000\n",
        "    submission.to_csv(DIR / f\"submissions/submission_{name}.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upV0y_MW9MpF"
      },
      "outputs": [],
      "source": [
        "test_model(\n",
        "    name=\"prithvi\",\n",
        "    checkpoint_path=LAST_CHECKPOINT_PATH,\n",
        "    input_channels=6,\n",
        "    nb_classes=20,\n",
        "    data_folder=Path(DATA_PATH_TEST),\n",
        "    batch_size=1,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "env_challenge",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
