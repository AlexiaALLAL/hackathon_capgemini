{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mines 2024 Data Challenge - Team 6 Documentation","text":"<p>Welcome to the Team 6 Data Challenge Documentation! Our researchs during the week are available on  GitHub . For the trainings we mainly used google Colab and a Google Drive</p>"},{"location":"#team-members","title":"Team members","text":"<ul> <li>Alexia ALLAL</li> <li>Jonas AMAR</li> <li>Guillaume BUTHMANN</li> <li>Fanny PICAMAL</li> </ul>"},{"location":"#scientific-report","title":"Scientific report","text":"<p>Our scientific report is available in scientific report section. It is a summary of our researchs and experiments during the week.</p>"},{"location":"#experiments","title":"Experiments","text":"<p>Here are the experiments we conducted during the week:</p>"},{"location":"#1-yolo-implementation","title":"1. YOLO implementation","text":"<p>You can find it in the <code>yolo/</code> folder. We tried to run YOLO on our data but quickly stopped due to YOLO being trained with polygonal bounding boxes for segmentation. Since our data is only annotated with the masks of the classes on our images, it is not possible to directly feed these annotations to YOLO.</p> <p>While trying to implement this code, we chose to give up on the temporal dimension, selecting only one image per sequence and choosing only the RBG channels in order to feed the images to YOLO.</p>"},{"location":"#2-implementation-of-a-vit-pretrained-on-a-crop-segmentation-task","title":"2. Implementation of a ViT pretrained on a crop segmentation task","text":"<p>You can find this implementation in <code>prithvi_notebook.ipynb</code>. Here, we used a promising network already pretrained and adaptated to our task, yet we weren't able to retrieve all the code, so we had to recode part of the network. It resulted in a working implementation that takes too long to train and gives poor results on the few epochs on which it was trained.</p>"},{"location":"#3-implementation-of-a-temporal-vit-from-scratch","title":"3. Implementation of a temporal ViT from scratch","text":"<p>You can find this implementation in <code>baseline\\TemporalVisionTransformer.py</code>. We tried to implement the time dependency starting from <code>torchvision</code>'s ViT model, but realized this wasn't going to be a successful approach given the amount of time we had.</p>"},{"location":"#4-implementation-of-a-simple-vision-transformer-whithout-time-dependency-from-scratch","title":"4. Implementation of a simple vision transformer whithout time dependency from scratch","text":"<p>You can find this implementation in <code>baseline\\SegmentationViT.py</code> and in the Drive folder. We tried to implement the simplest ViT we could using only one image by sequence. We had to modify the classification head of the network to fulfill the task of segmentation. This implementation uses the VisionTransformer from torchvision.models. In order to atapt it for our dataset, we had to change the number of channels in the ViT... (in_channels = 10) that was hardcoded in the original implementation. You can find the modified version in <code>baseline\\vision_transformer.py</code>.</p> <p>This model was trained on our data and resulted in a 8% mIoU on the visible part of the test set. It is our best performing implementation, yet we were able to get 10% mIoU by running only <code>baseline/model.py</code> on a few epochs, which shows the limit of this ViT network.</p>"},{"location":"SegmentationViT/","title":"4. Implementation of a simple vision transformer whithout time dependency from scratch","text":"<p>You can find this implementation in <code>baseline\\SegmentationViT.py</code> and in the Drive folder. We tried to implement the simplest ViT we could using only one image by sequence. We had to modify the classification head of the network to fulfill the task of segmentation. This implementation uses the VisionTransformer from torchvision.models. In order to atapt it for our dataset, we had to change the number of channels in the ViT... (in_channels = 10) that was hardcoded in the original implementation. You can find the modified version in <code>baseline\\vision_transformer.py</code>.</p> <p>This model was trained on our data and resulted in a 8% mIoU on the visible part of the test set. It is our best performing implementation, yet we were able to get 10% mIoU by running only <code>baseline/model.py</code> on a few epochs, which shows the limit of this ViT network.</p> <p>The file implements a <code>SegmentationViT</code> class, which is a Vision Transformer (ViT) model adapted for image segmentation tasks. Here\u2019s a structured documentation for this class and its components:</p>"},{"location":"SegmentationViT/#module-segmentationvit","title":"Module: <code>SegmentationViT</code>","text":""},{"location":"SegmentationViT/#overview","title":"Overview","text":"<p><code>SegmentationViT</code> is a PyTorch model designed for image segmentation tasks. This model leverages a Vision Transformer (ViT) as the backbone for feature extraction, specifically using only the 10th frame of a sequence as input. A lightweight decoder transforms the transformer outputs into a segmentation map.</p>"},{"location":"SegmentationViT/#dependencies","title":"Dependencies","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom baseline.vision_transformer import VisionTransformer\n</code></pre>"},{"location":"SegmentationViT/#class-segmentationvit","title":"Class: <code>SegmentationViT</code>","text":"<pre><code>class SegmentationViT(nn.Module):\n</code></pre>"},{"location":"SegmentationViT/#description","title":"Description","text":"<p>The <code>SegmentationViT</code> class inherits from <code>nn.Module</code>. It applies a ViT-based architecture tailored for image segmentation, with a decoder for mapping encoded features to a segmented output. The model is configured for a specified input size, patch size, number of input channels, and number of segmentation classes.</p>"},{"location":"SegmentationViT/#parameters","title":"Parameters","text":"<ul> <li><code>img_size</code> (int): Dimension of the input image, assumed square. Default is 128.</li> <li><code>patch_size</code> (int): Size of each patch divided from the image. Default is 16.</li> <li><code>in_channels</code> (int): Number of input channels in each image (e.g., 10 for a sequence). Default is 10.</li> <li><code>n_classes</code> (int): Number of segmentation classes. Default is 20.</li> </ul>"},{"location":"SegmentationViT/#attributes","title":"Attributes","text":"<ul> <li><code>vit</code> (<code>VisionTransformer</code>): The backbone model for feature extraction. The <code>VisionTransformer</code> component is imported from an external module, configured with:<ul> <li><code>image_size</code>: Input image size (<code>img_size</code>).</li> <li><code>patch_size</code>: Size of patches (<code>patch_size</code>).</li> <li><code>in_channels</code>: Number of input channels.</li> <li><code>num_layers</code>, <code>num_heads</code>, <code>hidden_dim</code>, and <code>mlp_dim</code>: Parameters defining transformer depth, attention heads, hidden dimensions, and feedforward layer size.</li> </ul> </li> <li><code>decoder</code> (<code>nn.Sequential</code>): Decoder to upsample the features and output a segmentation map. It consists of:<ul> <li><code>ConvTranspose2d</code>: Deconvolution layer for upsampling.</li> <li><code>ReLU</code>: Activation function.</li> <li>Additional convolutional and upsampling layers (depending on final dimensions) to map ViT output to segmentation labels.</li> </ul> </li> </ul>"},{"location":"SegmentationViT/#methods","title":"Methods","text":"<ul> <li><code>forward(x: torch.Tensor) -&gt; torch.Tensor</code><ul> <li>Forward pass for the segmentation model.</li> <li>Parameters:<ul> <li><code>x</code> (<code>torch.Tensor</code>): Input tensor of shape <code>(batch_size, in_channels, img_size, img_size)</code>.</li> </ul> </li> <li>Returns:<ul> <li><code>torch.Tensor</code>: Segmentation map output of shape <code>(batch_size, n_classes, img_size, img_size)</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"SegmentationViT/#example-usage","title":"Example Usage","text":"<pre><code>model = SegmentationViT(img_size=128, patch_size=16, in_channels=10, n_classes=20)\ninput_tensor = torch.randn(1, 10, 128, 128)\noutput = model(input_tensor)\nprint(output.shape)  # Expected output shape: (1, 20, 128, 128)\n</code></pre>"},{"location":"SegmentationViT/#notes","title":"Notes","text":"<ul> <li>This model is specifically structured to use only the 10th frame from a sequence, meaning it may need adaptation for real-time or frame-dependent applications.</li> <li>The ViT is used as a feature extractor, while the decoder is designed to handle the transformation of features into a spatial segmentation map.</li> </ul>"},{"location":"TemporalVisionTransformer/","title":"3. Implementation of a temporal ViT from scratch","text":"<p>You can find this implementation in <code>baseline\\TemporalVisionTransformer.py</code>. We tried to implement the time dependency starting from <code>torchvision</code>'s ViT model, but realized this wasn't going to be a successful approach given the amount of time we had.</p> <p>The file contains a <code>TemporalVisionTransformer</code> class, which adapts a Vision Transformer for processing temporal sequences of images. Here\u2019s a structured documentation for this class and its components:</p>"},{"location":"TemporalVisionTransformer/#module-temporalvisiontransformer","title":"Module: <code>TemporalVisionTransformer</code>","text":""},{"location":"TemporalVisionTransformer/#overview","title":"Overview","text":"<p><code>TemporalVisionTransformer</code> is a PyTorch model designed to handle temporal image sequences. This model leverages a Vision Transformer (ViT) as the backbone for feature extraction, with added support for temporal sequence processing, allowing it to capture patterns across time as well as spatial features.</p>"},{"location":"TemporalVisionTransformer/#dependencies","title":"Dependencies","text":"<pre><code>from transformers import AutoImageProcessor, ViTModel\nimport torch\nimport torch.nn as nn\nfrom baseline.vision_transformer import VisionTransformer\nfrom einops import rearrange\n</code></pre>"},{"location":"TemporalVisionTransformer/#class-temporalvisiontransformer","title":"Class: <code>TemporalVisionTransformer</code>","text":"<pre><code>class TemporalVisionTransformer(nn.Module):\n</code></pre>"},{"location":"TemporalVisionTransformer/#description","title":"Description","text":"<p>The <code>TemporalVisionTransformer</code> class is a neural network model for temporal image sequence analysis, inheriting from <code>nn.Module</code>. It integrates a Vision Transformer (ViT) adapted for temporal data processing, with positional encoding for the temporal dimension to encode time-related information.</p>"},{"location":"TemporalVisionTransformer/#parameters","title":"Parameters","text":"<ul> <li><code>img_size</code> (int): Size of the input image (assumed square). Default is 128.</li> <li><code>patch_size</code> (int): Size of patches within the image. Default is 16.</li> <li><code>in_channels</code> (int): Number of input channels per image in the sequence. Default is 10.</li> <li><code>seq_length</code> (int): Number of images in the sequence. Default is 61.</li> <li><code>embed_dim</code> (int): Embedding dimension for transformer. Default is 768.</li> <li><code>num_heads</code> (int): Number of attention heads in each transformer layer. Default is 5.</li> <li><code>num_layers</code> (int): Number of layers in the transformer. Default is 6.</li> <li><code>num_classes</code> (int): Number of output classes. Default is 1.</li> </ul>"},{"location":"TemporalVisionTransformer/#attributes","title":"Attributes","text":"<ul> <li><code>vit</code> (<code>VisionTransformer</code>): The core Vision Transformer module for extracting features from each image.<ul> <li>Configured with <code>img_size</code>, <code>patch_size</code>, <code>num_layers</code>, <code>num_heads</code>, <code>in_channels</code>, <code>embed_dim</code>, and <code>num_classes</code>.</li> </ul> </li> <li><code>seq_length</code> (int): Length of the temporal sequence.</li> <li><code>embed_dim</code> (int): Dimension for embedding in the temporal context.</li> <li><code>temporal_positional_encoding</code> (<code>nn.Parameter</code>): Positional encoding for each frame in the sequence, enhancing temporal awareness in the transformer model.</li> <li><code>decoder</code> (<code>nn.Sequential</code>): Decoder to upsample the features and output a segmentation map</li> </ul>"},{"location":"TemporalVisionTransformer/#methods","title":"Methods","text":"<ul> <li><code>forward(x: torch.Tensor) -&gt; torch.Tensor</code><ul> <li>Forward pass for the temporal Vision Transformer.</li> <li>Parameters:<ul> <li><code>x</code> (<code>torch.Tensor</code>): Input tensor of shape <code>(batch_size, seq_length, in_channels, img_size, img_size)</code>.</li> </ul> </li> <li>Returns:<ul> <li><code>torch.Tensor</code>: Output tensor for classification or regression tasks, typically of shape <code>(batch_size, num_classes)</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"TemporalVisionTransformer/#example-usage","title":"Example Usage","text":"<pre><code>model = TemporalVisionTransformer(img_size=128, patch_size=16, in_channels=10, seq_length=61, embed_dim=768)\ninput_tensor = torch.randn(1, 61, 10, 128, 128)  # Example input\noutput = model(input_tensor)\nprint(output.shape)  # Expected output shape depends on num_classes (default is (1, 1))\n</code></pre>"},{"location":"TemporalVisionTransformer/#notes","title":"Notes","text":"<ul> <li>This model includes a ViT backbone for spatial feature extraction, with adjustments for handling sequences in the temporal dimension.</li> <li>Temporal positional encoding would allows the model to capture dependencies across the sequence length, which may improve performance on tasks involving sequential data.</li> </ul>"},{"location":"YOLO/","title":"1. YOLO implementation","text":"<p>You can find it in the <code>yolo/</code> folder. We tried to run YOLO on our data but quickly stopped due to YOLO being trained with polygonal bounding boxes for segmentation. Since our data is only annotated with the masks of the classes on our images, it is not possible to directly feed these annotations to YOLO.</p> <p>While trying to implement this code, we chose to give up on the temporal dimension, selecting only one image per sequence and choosing only the RBG channels in order to feed the images to YOLO.</p>"},{"location":"prithvi/","title":"2. Implementation of a ViT pretrained on a crop segmentation task","text":"<p>You can find this implementation in <code>prithvi_notebook.ipynb</code>. Here, we used a promising network already pretrained and adaptated to our task, yet we weren't able to retrieve all the code, so we had to recode part of the network. It resulted in a working implementation that takes too long to train and gives poor results on the few epochs on which it was trained.</p>"},{"location":"task/","title":"Description of the task","text":""},{"location":"task/#rapport-scientifique","title":"Rapport scientifique","text":""},{"location":"task/#contexte-du-projet-et-description","title":"Contexte du projet et description","text":"<p>Avec la croissance d\u00e9mographique mondiale et les d\u00e9fis li\u00e9s au changement climatique, il est devenu crucial de g\u00e9rer efficacement les ressources agricoles pour garantir la s\u00e9curit\u00e9 alimentaire et pr\u00e9server l'environnement. La t\u00e9l\u00e9d\u00e9tection par satellite offre une solution moderne et efficace pour la surveillance des cultures, en permettant une analyse r\u00e9guli\u00e8re et \u00e9tendue des terres agricoles sans n\u00e9cessiter de visites sur le terrain. En observant la dynamique des cultures \u00e0 partir d'images satellites, les d\u00e9cideurs peuvent obtenir des informations cl\u00e9s sur les types de cultures plant\u00e9es, leur \u00e9tat de sant\u00e9, ainsi que sur les pratiques agricoles adopt\u00e9es par les agriculteurs, afin de suivre l'\u00e9volution de l'utilisation des terres au fil du temps. Cela rev\u00eat une importance capitale pour optimiser l'allocation des ressources, am\u00e9liorer la gestion des cultures, et garantir le respect des politiques agricoles telles que la politique agricole commune (PAC).</p> <p>L'objectif principal du projet est de d\u00e9velopper un syst\u00e8me automatique capable de segmenter et de classifier des parcelles agricoles en fonction des types de cultures pr\u00e9sentes, en utilisant des s\u00e9ries temporelles d'images satellites. Contrairement \u00e0 une simple analyse d'image statique, les s\u00e9ries temporelles permettent de suivre les variations saisonni\u00e8res et ph\u00e9nologiques (changements au cours du cycle de vie des cultures) sur une ann\u00e9e enti\u00e8re, ce qui aide \u00e0 distinguer les cultures entre elles et \u00e0 d\u00e9tecter les cultures interm\u00e9diaires ou les jach\u00e8res. En utilisant ces donn\u00e9es, le mod\u00e8le peut g\u00e9n\u00e9rer des pr\u00e9dictions pr\u00e9cises sur la nature des cultures pr\u00e9sentes, permettant ainsi une prise de d\u00e9cision plus \u00e9clair\u00e9e et de meilleurs r\u00e9sultats commerciaux et environnementaux.</p> <p>La pr\u00e9cision des pr\u00e9dictions issues de ce projet peut avoir des applications multiples :</p> <ul> <li> <p>Optimisation de la gestion agricole : Aider les agriculteurs \u00e0 pr\u00e9voir les besoins en irrigation, en engrais et en pesticides, en fonction du type de culture et de son \u00e9tat de sant\u00e9.</p> </li> <li> <p>Suivi des politiques agricoles : Permettre aux institutions gouvernementales de v\u00e9rifier le respect des r\u00e9glementations agricoles, notamment celles li\u00e9es aux subventions et aux aides de la PAC.</p> </li> <li> <p>Pr\u00e9diction et analyse des rendements : Fournir des estimations pr\u00e9coces des rendements pour planifier la cha\u00eene d'approvisionnement et anticiper les prix des denr\u00e9es alimentaires sur le march\u00e9.</p> </li> <li> <p>Environnement et durabilit\u00e9 : Faciliter la surveillance de l'utilisation des terres et de la biodiversit\u00e9, et rep\u00e9rer les pratiques agricoles non durables telles que la monoculture excessive.</p> </li> <li> <p>\u00c9tude du changement climatique : suivre de mani\u00e8re r\u00e9troactive l\u2019\u00e9volution des pratiques agricoles dans un contexte de r\u00e9chauffement climatique afin de mieux comprendre et agir sur ces changements.</p> </li> </ul> <p>Pour ce faire, le projet utilise des donn\u00e9es satellites multi-spectrales (captant la lumi\u00e8re dans 10 longueurs d'ondes diff\u00e9rentes), qui permettent de diff\u00e9rencier les cultures gr\u00e2ce \u00e0 leur signature spectrale et leur p\u00e9riodicit\u00e9 annuelle unique. Le mod\u00e8le d\u00e9velopp\u00e9, un Vision Transformer adapt\u00e9 aux t\u00e2ches de segmentation et d'analyse temporelle, exploite ces informations pour fournir des pr\u00e9dictions pr\u00e9cises et robustes.</p>"},{"location":"task/#scope-du-projet","title":"Scope du projet","text":"<p>Le projet se concentre sur la conception et la mise en \u0153uvre d'un mod\u00e8le capable de segmenter des images satellites bas\u00e9es sur des s\u00e9ries temporelles et de classifier les parcelles agricoles par type de culture. Pour y parvenir, plusieurs \u00e9tapes ont \u00e9t\u00e9 d\u00e9finies :</p>"},{"location":"task/#1-preparation-des-donnees","title":"1. Pr\u00e9paration des donn\u00e9es :","text":"<p>Les donn\u00e9es d'entr\u00e9e consistent en des images satellites couvrant des zones de 10x10 km, captur\u00e9es tout au long de l'ann\u00e9e. Chaque image comprend 10 bandes spectrales (par exemple, rouge, vert, bleu, proche infrarouge, etc.) qui fournissent des informations riches sur la v\u00e9g\u00e9tation et le sol.</p> <p>Les donn\u00e9es en sortie sont une segmentation de chaque image avec 20 classes possibles de types de cultures, chaque classe correspondant \u00e0 un type de culture sp\u00e9cifique.</p>"},{"location":"task/#2-developpement-du-modele","title":"2. D\u00e9veloppement du mod\u00e8le :","text":"<p>Le choix d'un Vision Transformer (ViT) a \u00e9t\u00e9 motiv\u00e9 par sa capacit\u00e9 \u00e0 traiter des donn\u00e9es  tout en tenant compte des relations spatiales et temporelles complexes, notamment dans le cas de s\u00e9ries d\u2019images. Contrairement aux r\u00e9seaux convolutifs traditionnels, le ViT utilise des m\u00e9canismes d'attention pour capturer les d\u00e9pendances \u00e0 long terme dans les donn\u00e9es, ce qui le rend particuli\u00e8rement adapt\u00e9 aux s\u00e9ries temporelles. L'architecture du mod\u00e8le a \u00e9t\u00e9 adapt\u00e9e pour int\u00e9grer des s\u00e9quences temporelles d'images en entr\u00e9e. Cela permet au mod\u00e8le de comprendre l'\u00e9volution des cultures au fil du temps, plut\u00f4t que de se baser uniquement sur un instantan\u00e9 statique.</p>"},{"location":"task/#3-entrainement-et-validation","title":"3. Entra\u00eenement et validation :","text":"<p>Pour \u00e9valuer la performance du mod\u00e8le de segmentation, nous avons utilis\u00e9 des m\u00e9triques couramment employ\u00e9es en analyse d'images : la mean IoU (Intersection over Union).</p> <p>Cette m\u00e9trique mesure la similitude entre la pr\u00e9diction du mod\u00e8le et la v\u00e9rit\u00e9 terrain. Pour chaque classe, l'IoU est calcul\u00e9e en divisant l'aire de l'intersection (les pixels correctement pr\u00e9dits comme appartenant \u00e0 cette classe) par l'aire de l'union (tous les pixels pr\u00e9dits comme appartenant \u00e0 la classe, plus ceux qui devraient l'\u00eatre mais qui ne le sont pas). Formellement, cela se traduit par :</p> <p>IoU = Aire de l'intersection / Aire de l'union</p> <p>Plus la valeur de l'IoU est \u00e9lev\u00e9e, plus la pr\u00e9diction est proche de la r\u00e9alit\u00e9. La mean IoU est la moyenne des IoU calcul\u00e9es pour toutes les classes de culture, offrant une mesure globale de la performance du mod\u00e8le.</p> <p>Une difficult\u00e9 majeure dans la classification des cultures r\u00e9side dans la gestion des classes de cultures sous-repr\u00e9sent\u00e9es. Si certaines classes sont tr\u00e8s fr\u00e9quentes dans les donn\u00e9es, d'autres sont beaucoup plus rares. Cela peut entra\u00eener un d\u00e9s\u00e9quilibre : le mod\u00e8le pourrait ignorer ces classes peu repr\u00e9sent\u00e9es ou avoir du mal \u00e0 les classifier correctement, ce qui serait probl\u00e9matique si des parcelles rares n'\u00e9taient jamais bien identifi\u00e9es.</p> <p>Pour rem\u00e9dier \u00e0 ce probl\u00e8me, nous pouvons appliquer une pond\u00e9ration des classes lors de l'\u00e9valuation du mod\u00e8le. Cette technique consiste \u00e0 augmenter l'importance des erreurs commises sur les classes rares. En d'autres termes, si le mod\u00e8le se trompe sur une parcelle de culture rare, cette erreur est plus \"co\u00fbteuse\" que si la m\u00eame erreur avait \u00e9t\u00e9 commise sur une classe fr\u00e9quente. Cela incite le mod\u00e8le \u00e0 accorder plus d'attention aux cultures moins repr\u00e9sent\u00e9es et \u00e0 apprendre \u00e0 mieux les reconna\u00eetre, assurant ainsi une classification plus pr\u00e9cise et plus \u00e9quitable de toutes les parcelles agricoles, m\u00eame les plus rares.</p> <p>Ce processus devrait permettre d'obtenir un mod\u00e8le plus robuste et capable de g\u00e9n\u00e9raliser sur l'ensemble des types de cultures pr\u00e9sents dans les donn\u00e9es, garantissant que toutes les cat\u00e9gories de parcelles soient bien prises en compte, quel que soit leur niveau de repr\u00e9sentativit\u00e9.</p>"},{"location":"task/#presentation-du-groupe","title":"Pr\u00e9sentation du groupe","text":"<p>Le projet a \u00e9t\u00e9 r\u00e9alis\u00e9 par un groupe de quatre \u00e9tudiants en derni\u00e8re ann\u00e9e \u00e0 l\u2019\u00c9cole des Mines de Paris, dans le cadre d\u2019un data challenge organis\u00e9 par Capgemini. Le challenge nous a offert l\u2019opportunit\u00e9 d\u2019aborder des probl\u00e8mes concrets en utilisant des techniques avanc\u00e9es de data science et d\u2019intelligence artificielle. Le groupe a combin\u00e9 ses comp\u00e9tences en Deep Learning, traitement d'images, et analyse de s\u00e9ries temporelles pour relever ce d\u00e9fi ambitieux.</p>"},{"location":"task/#gestion-des-taches","title":"Gestion des t\u00e2ches","text":"<p>Le projet s'est d\u00e9roul\u00e9 sur une p\u00e9riode d\u2019une semaine intensive. Pour assurer une coordination efficace, l'\u00e9quipe a travaill\u00e9 majoritairement en pr\u00e9sentiel durant la journ\u00e9e, permettant de discuter des choix techniques, de r\u00e9soudre les probl\u00e8mes rencontr\u00e9s, et de tester rapidement de nouvelles id\u00e9es. Le soir, chaque membre poursuivait le travail de son c\u00f4t\u00e9, afin de progresser sur des aspects sp\u00e9cifiques du projet.</p> <p>Afin de faciliter la collaboration et le partage de code, nous avons utilis\u00e9 un  repository GitHub  pour centraliser le d\u00e9veloppement. Chaque membre pouvait ainsi int\u00e9grer ses contributions, permettant un suivi continu et une mise \u00e0 jour r\u00e9guli\u00e8re du projet. Cette m\u00e9thode de travail en \u00e9quipe a favoris\u00e9 une grande r\u00e9activit\u00e9 et a permis de produire un prototype fonctionnel en un temps tr\u00e8s court.</p> <p>Nous avons aussi essay\u00e9 de parall\u00e9liser au maximum les recherches, et quand c\u2019\u00e9tait impossible, de lancer des recherches dans plusieurs directions diff\u00e9rentes en m\u00eame temps, quitte \u00e0 abandonner rapidement celles qui n\u2019auraient pas port\u00e9 de fruits. Cette m\u00e9thode nous \u00e0 permis d\u2019explorer plusieurs architectures en parall\u00e8le et de conclure sur les avantages et les inconv\u00e9nients de chacune d\u2019entre elles.</p>"},{"location":"task/#gestion-de-projet","title":"Gestion de projet","text":""},{"location":"task/#comprehension-des-donnees","title":"Compr\u00e9hension des donn\u00e9es","text":"<p>La premi\u00e8re \u00e9tape du projet a consist\u00e9 \u00e0 comprendre en profondeur la structure et la nature des donn\u00e9es disponibles. Les donn\u00e9es fournies comprenaient des s\u00e9ries temporelles d'images satellites, repr\u00e9sentant des zones de 10x10 km tout au long d'une ann\u00e9e. Chaque image contient des informations sur 10 bandes spectrales diff\u00e9rentes, telles que le rouge, le vert, le bleu, et le proche infrarouge. Ces bandes spectrales permettent d'observer divers aspects des cultures, comme la sant\u00e9 v\u00e9g\u00e9tale ou l'humidit\u00e9 du sol, \u00e0 diff\u00e9rentes p\u00e9riodes de l'ann\u00e9e.</p> <p>Les donn\u00e9es d'entr\u00e9e peuvent \u00eatre organis\u00e9es sous la forme d\u2019un tenseur de taille (N, T, C, H, W) :</p> <ul> <li> <p>N repr\u00e9sente le nombre de diff\u00e9rentes zones g\u00e9ographiques observ\u00e9es (nombre de parcelles),</p> </li> <li> <p>T est le nombre de fois o\u00f9 chaque zone a \u00e9t\u00e9 photographi\u00e9e au cours de l'ann\u00e9e. Pour garantir une coh\u00e9rence des donn\u00e9es, toutes les s\u00e9ries temporelles ont \u00e9t\u00e9 compl\u00e9t\u00e9es par des images nulles lorsque n\u00e9cessaire, de mani\u00e8re \u00e0 obtenir exactement T=61 images pour chaque zone,</p> </li> <li> <p>C est le nombre de canaux spectraux (bandes spectrales), avec C=10 pour les 10 longueurs d'onde captur\u00e9es par les satellites,</p> </li> <li> <p>H et W sont respectivement la hauteur et la largeur des images, fix\u00e9es \u00e0 128x128 pixels.</p> </li> </ul> <p>Le mod\u00e8le de segmentation que nous avons d\u00e9velopp\u00e9 prend ce tenseur comme entr\u00e9e et produit en sortie un tenseur de taille (B, H, W), o\u00f9 B repr\u00e9sente le batch size utilis\u00e9 pendant l\u2019entra\u00eenement ou la pr\u00e9diction, et H et W correspondent toujours \u00e0 la taille des images de 128x128 pixels. Chaque pixel de la sortie est associ\u00e9 \u00e0 l'une des 20 classes de types de culture, permettant ainsi de g\u00e9n\u00e9rer des cartes de segmentation des cultures sur chaque parcelle analys\u00e9e.</p> <p>Cette structure de donn\u00e9es claire et standardis\u00e9e a facilit\u00e9 la manipulation et l'analyse des s\u00e9ries temporelles complexes</p>"},{"location":"task/#pretraitement-des-donnees","title":"Pr\u00e9traitement des donn\u00e9es","text":"<p>Le pr\u00e9traitement des donn\u00e9es est une \u00e9tape essentielle dans tout projet de mod\u00e9lisation, mais dans notre cas, cette phase a \u00e9t\u00e9 simplifi\u00e9e gr\u00e2ce \u00e0 la qualit\u00e9 des donn\u00e9es fournies. Les donn\u00e9es \u00e9taient d\u00e9j\u00e0 nettoy\u00e9es et pr\u00e9trait\u00e9es par Capgemini, ce qui a grandement facilit\u00e9 notre travail. Nous n\u2019avons pas eu besoin de proc\u00e9der \u00e0 des \u00e9tapes suppl\u00e9mentaires de filtrage ou de normalisation, et avons ainsi pu ins\u00e9rer directement les donn\u00e9es dans notre mod\u00e8le pour l'entra\u00eenement et la validation.</p> <p>Cette situation nous a permis de nous concentrer davantage sur les aspects de mod\u00e9lisation et d\u2019optimisation du mod\u00e8le, plut\u00f4t que sur le traitement des donn\u00e9es brutes.</p>"},{"location":"task/#modelisation","title":"Mod\u00e9lisation","text":"<p>La mod\u00e9lisation a constitu\u00e9 le c\u0153ur du projet. Pour cette t\u00e2che, nous avons opt\u00e9 pour un Vision Transformer (ViT), adapt\u00e9 pour traiter \u00e0 la fois des images et des s\u00e9ries temporelles complexes. </p> <p>Voici les principales \u00e9tapes de cette phase :</p> <ul> <li> <p>Choix du mod\u00e8le : Le ViT a \u00e9t\u00e9 choisi pour sa capacit\u00e9 \u00e0 exploiter des m\u00e9canismes d'attention permettant de capturer les relations spatiales et temporelles entre les diff\u00e9rents pixels d\u2019une image. Contrairement aux r\u00e9seaux de neurones convolutifs traditionnels, le ViT offre une plus grande flexibilit\u00e9 pour apprendre des d\u00e9pendances \u00e0 long terme, ce qui est essentiel pour notre t\u00e2che de segmentation des cultures.</p> </li> <li> <p>Difficult\u00e9s et adaptations n\u00e9cessaires : La principale difficult\u00e9 rencontr\u00e9e durant cette phase a \u00e9t\u00e9 d\u2019adapter le ViT pour r\u00e9pondre aux sp\u00e9cificit\u00e9s de notre projet :</p> <ul> <li> <p>Adaptation \u00e0 la segmentation d\u2019images : Par d\u00e9faut, le ViT est con\u00e7u pour classifier des images compl\u00e8tes en une seule cat\u00e9gorie. Nous avons d\u00fb modifier sa structure pour qu'il puisse produire une classe par pixel de l\u2019image d\u2019entr\u00e9e, permettant ainsi de g\u00e9n\u00e9rer des cartes de segmentation pr\u00e9cises des parcelles agricoles.</p> </li> <li> <p>Prise en compte des 10 canaux : Contrairement aux images standard en RGB (3 canaux), nos donn\u00e9es contiennent 10 bandes spectrales. Il a fallu adapter le mod\u00e8le pour qu'il puisse int\u00e9grer et traiter ces 10 canaux simultan\u00e9ment, en s'assurant que chaque bande spectrale contribue efficacement \u00e0 la segmentation.</p> </li> <li> <p>Adaptation aux s\u00e9ries temporelles : Le mod\u00e8le devait \u00e9galement comprendre la dynamique temporelle des images, c\u2019est-\u00e0-dire la fa\u00e7on dont les cultures \u00e9voluent au fil du temps. Pendant nos recherches, nous avons tent\u00e9 de modifier le ViT pour qu'il prenne en entr\u00e9e des s\u00e9quences temporelles d'images, permettant de capturer ces variations et d'am\u00e9liorer la pr\u00e9cision de la pr\u00e9diction finale. La technique choisie a \u00e9t\u00e9 de concat\u00e9ner les embeddings des images de chaque s\u00e9rie temporelle pour former un embedding de la s\u00e9rie. Malheureusement, le mod\u00e8le ainsi d\u00e9velopp\u00e9 prend des dimensions trop grandes pour que notre puissance de calcul suffise \u00e0 l'entra\u00eener</p> </li> </ul> </li> <li> <p>Entra\u00eenement et \u00e9valuation : Pour entra\u00eener le mod\u00e8le, nous avons utilis\u00e9 le jeu de donn\u00e9es \u00e9tiquet\u00e9 fourni par Capgemini comprenant diff\u00e9rentes s\u00e9ries temporelles d'images satellites. Les performances ont \u00e9t\u00e9 mesur\u00e9es \u00e0 l\u2019aide de m\u00e9triques telles que la mean IoU, en tenant compte des pond\u00e9rations pour les classes rares, afin de s'assurer que le mod\u00e8le pouvait d\u00e9tecter m\u00eame les types de cultures les moins repr\u00e9sent\u00e9s.</p> </li> </ul>"},{"location":"task/#strategie-de-deploiement","title":"Strat\u00e9gie de d\u00e9ploiement","text":"<p>L\u2019entra\u00eenement du mod\u00e8le n\u00e9cessitant une puissance de calcul importante, nous avons utilis\u00e9 des GPU pour acc\u00e9l\u00e9rer le processus. En raison des limitations de nos ordinateurs personnels, nous avons opt\u00e9 pour Google Colab et Kaggle, deux plateformes offrant un acc\u00e8s gratuit \u00e0 des GPUs. Cette solution nous a permis d\u2019entra\u00eener nos mod\u00e8les efficacement et de r\u00e9aliser plusieurs it\u00e9rations pour am\u00e9liorer les performances du mod\u00e8le.</p> <p>Pour la suite du d\u00e9ploiement, nous envisageons de designer une interface utilisateur permettant \u00e0 un utilisateur peu familier avec les environnement de code d\u2019obtenir des r\u00e9sultats. Comme le mod\u00e8le est destin\u00e9 \u00e0 \u00eatre utilis\u00e9 par un nombre relativement restreint d\u2019utilisateurs, une simple interface devrait suffire \u00e0 son utilisation.</p>"},{"location":"task/#conclusion","title":"Conclusion","text":"<p>Lors de ce projet, notre \u00e9quipe a \u00e9t\u00e9 amen\u00e9e \u00e0 explorer diff\u00e9rentes technologies \u00e0 m\u00eame de produire des r\u00e9sultats dans un temps limit\u00e9 et avec une puissance de calcul restreinte. Le travail de groupe \u00e0 \u00e9t\u00e9 source d\u2019innovation et \u00e0 permis d\u2019explorer plusieurs architectures pour le mod\u00e8le de mani\u00e8re simultan\u00e9e. Finalement, l\u2019architecture choisie est un ViT de classification, adapt\u00e9 \u00e0 la t\u00e2che de segmentation de notre jeu de donn\u00e9es. Si les performances de notre mod\u00e8le (environ 8% d\u2019IoU sur le jeu de test) ne sont pas suffisantes pour qu\u2019il serve son but initial, elles ont probablement \u00e9t\u00e9 brid\u00e9es par le manque de temps de d\u00e9veloppement et d'entra\u00eenement auquel nous avons d\u00fb faire face. </p> <p>Ainsi, les axes de recherches toujours actifs sont de parvenir \u00e0 impl\u00e9menter la d\u00e9pendance spatiale dans notre mod\u00e8le de mani\u00e8re optimis\u00e9e (le blocage aujourd\u2019hui r\u00e9side dans la puissance de calcul n\u00e9cessaire \u00e0 entra\u00eener notre impl\u00e9mentation), ainsi que de parvenir \u00e0 impl\u00e9menter un ViT pr\u00e9 entra\u00een\u00e9 sur une t\u00e2che de segmentation sur des images satellites, afin de capitaliser sur les recherches pass\u00e9es.</p>"}]}