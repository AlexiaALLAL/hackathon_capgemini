{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mines 2024 Data Challenge - Team 6 Documentation","text":"<p>Welcome to the Team 6 Data Challenge Documentation! Our researchs during the week are available on GitHub GitHub . For the trainings we mainly used google Colab and a Google Drive link to the drive : </p>"},{"location":"#experiments","title":"Experiments","text":"<p>Here are the experiments we conducted during the week:</p>"},{"location":"#1-yolo-implementation","title":"1. YOLO implementation","text":"<p>You can find it in the <code>yolo/</code> folder. We tried to run YOLO on our data but quickly stopped due to YOLO being trained with polygonal bounding boxes for segmentation. Since our data is only annotated with the masks of the classes on our images, it is not possible to directly feed these annotations to YOLO.</p> <p>While trying to implement this code, we chose to give up on the temporal dimension, selecting only one image per sequence and choosing only the RBG channels in order to feed the images to YOLO.</p>"},{"location":"#2-implementation-of-a-vit-pretrained-on-a-crop-segmentation-task","title":"2. Implementation of a ViT pretrained on a crop segmentation task","text":"<p>You can find this implementation in <code>prithvi_notebook.ipynb</code>. Here, we used a promising network already pretrained and adaptated to our task, yet we weren't able to retrieve all the code, so we had to recode part of the network. It resulted in a working implementation that takes too long to train and gives poor results on the few epochs on which it was trained.</p>"},{"location":"#3-implementation-of-a-temporal-vit-from-scratch","title":"3. Implementation of a temporal ViT from scratch","text":"<p>You can find this implementation in <code>baseline\\TemporalVisionTransformer.py</code>. We tried to implement the time dependency starting from <code>torchvision</code>'s ViT model, but realized this wasn't going to be a successful approach given the amount of time we had.</p>"},{"location":"#4-implementation-of-a-simple-vision-transformer-whithout-time-dependency-from-scratch","title":"4. Implementation of a simple vision transformer whithout time dependency from scratch","text":"<p>You can find this implementation in <code>baseline\\SegmentationViT.py</code> and in the Drive folder. We tried to implement the simplest ViT we could using only one image by sequence. We had to modify the classification head of the network to fulfill the task of segmentation. This implementation uses the VisionTransformer from torchvision.models. In order to atapt it for our dataset, we had to change the number of channels in the ViT... (in_channels = 10) that was hardcoded in the original implementation. You can find the modified version in <code>baseline\\vision_transformer.py</code>.</p> <p>This model was trained on our data and resulted in a 8% mIoU on the visible part of the test set. It is our best performing implementation, yet we were able to get 10% mIoU by running only <code>baseline/model.py</code> on a few epochs, which shows the limit of this ViT network.</p>"},{"location":"SegmentationViT/","title":"4. Implementation of a simple vision transformer whithout time dependency from scratch","text":"<p>You can find this implementation in <code>baseline\\SegmentationViT.py</code> and in the Drive folder. We tried to implement the simplest ViT we could using only one image by sequence. We had to modify the classification head of the network to fulfill the task of segmentation. This implementation uses the VisionTransformer from torchvision.models. In order to atapt it for our dataset, we had to change the number of channels in the ViT... (in_channels = 10) that was hardcoded in the original implementation. You can find the modified version in <code>baseline\\vision_transformer.py</code>.</p> <p>This model was trained on our data and resulted in a 8% mIoU on the visible part of the test set. It is our best performing implementation, yet we were able to get 10% mIoU by running only <code>baseline/model.py</code> on a few epochs, which shows the limit of this ViT network.</p> <p>The file implements a <code>SegmentationViT</code> class, which is a Vision Transformer (ViT) model adapted for image segmentation tasks. Here\u2019s a structured documentation for this class and its components:</p>"},{"location":"SegmentationViT/#module-segmentationvit","title":"Module: <code>SegmentationViT</code>","text":""},{"location":"SegmentationViT/#overview","title":"Overview","text":"<p><code>SegmentationViT</code> is a PyTorch model designed for image segmentation tasks. This model leverages a Vision Transformer (ViT) as the backbone for feature extraction, specifically using only the 10th frame of a sequence as input. A lightweight decoder transforms the transformer outputs into a segmentation map.</p>"},{"location":"SegmentationViT/#dependencies","title":"Dependencies","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom baseline.vision_transformer import VisionTransformer\n</code></pre>"},{"location":"SegmentationViT/#class-segmentationvit","title":"Class: <code>SegmentationViT</code>","text":"<pre><code>class SegmentationViT(nn.Module):\n</code></pre>"},{"location":"SegmentationViT/#description","title":"Description","text":"<p>The <code>SegmentationViT</code> class inherits from <code>nn.Module</code>. It applies a ViT-based architecture tailored for image segmentation, with a decoder for mapping encoded features to a segmented output. The model is configured for a specified input size, patch size, number of input channels, and number of segmentation classes.</p>"},{"location":"SegmentationViT/#parameters","title":"Parameters","text":"<ul> <li><code>img_size</code> (int): Dimension of the input image, assumed square. Default is 128.</li> <li><code>patch_size</code> (int): Size of each patch divided from the image. Default is 16.</li> <li><code>in_channels</code> (int): Number of input channels in each image (e.g., 10 for a sequence). Default is 10.</li> <li><code>n_classes</code> (int): Number of segmentation classes. Default is 20.</li> </ul>"},{"location":"SegmentationViT/#attributes","title":"Attributes","text":"<ul> <li><code>vit</code> (<code>VisionTransformer</code>): The backbone model for feature extraction. The <code>VisionTransformer</code> component is imported from an external module, configured with:<ul> <li><code>image_size</code>: Input image size (<code>img_size</code>).</li> <li><code>patch_size</code>: Size of patches (<code>patch_size</code>).</li> <li><code>in_channels</code>: Number of input channels.</li> <li><code>num_layers</code>, <code>num_heads</code>, <code>hidden_dim</code>, and <code>mlp_dim</code>: Parameters defining transformer depth, attention heads, hidden dimensions, and feedforward layer size.</li> </ul> </li> <li><code>decoder</code> (<code>nn.Sequential</code>): Decoder to upsample the features and output a segmentation map. It consists of:<ul> <li><code>ConvTranspose2d</code>: Deconvolution layer for upsampling.</li> <li><code>ReLU</code>: Activation function.</li> <li>Additional convolutional and upsampling layers (depending on final dimensions) to map ViT output to segmentation labels.</li> </ul> </li> </ul>"},{"location":"SegmentationViT/#methods","title":"Methods","text":"<ul> <li><code>forward(x: torch.Tensor) -&gt; torch.Tensor</code><ul> <li>Forward pass for the segmentation model.</li> <li>Parameters:<ul> <li><code>x</code> (<code>torch.Tensor</code>): Input tensor of shape <code>(batch_size, in_channels, img_size, img_size)</code>.</li> </ul> </li> <li>Returns:<ul> <li><code>torch.Tensor</code>: Segmentation map output of shape <code>(batch_size, n_classes, img_size, img_size)</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"SegmentationViT/#example-usage","title":"Example Usage","text":"<pre><code>model = SegmentationViT(img_size=128, patch_size=16, in_channels=10, n_classes=20)\ninput_tensor = torch.randn(1, 10, 128, 128)\noutput = model(input_tensor)\nprint(output.shape)  # Expected output shape: (1, 20, 128, 128)\n</code></pre>"},{"location":"SegmentationViT/#notes","title":"Notes","text":"<ul> <li>This model is specifically structured to use only the 10th frame from a sequence, meaning it may need adaptation for real-time or frame-dependent applications.</li> <li>The ViT is used as a feature extractor, while the decoder is designed to handle the transformation of features into a spatial segmentation map.</li> </ul>"},{"location":"TemporalVisionTransformer/","title":"3. Implementation of a temporal ViT from scratch","text":"<p>You can find this implementation in <code>baseline\\TemporalVisionTransformer.py</code>. We tried to implement the time dependency starting from <code>torchvision</code>'s ViT model, but realized this wasn't going to be a successful approach given the amount of time we had.</p> <p>The file contains a <code>TemporalVisionTransformer</code> class, which adapts a Vision Transformer for processing temporal sequences of images. Here\u2019s a structured documentation for this class and its components:</p>"},{"location":"TemporalVisionTransformer/#module-temporalvisiontransformer","title":"Module: <code>TemporalVisionTransformer</code>","text":""},{"location":"TemporalVisionTransformer/#overview","title":"Overview","text":"<p><code>TemporalVisionTransformer</code> is a PyTorch model designed to handle temporal image sequences. This model leverages a Vision Transformer (ViT) as the backbone for feature extraction, with added support for temporal sequence processing, allowing it to capture patterns across time as well as spatial features.</p>"},{"location":"TemporalVisionTransformer/#dependencies","title":"Dependencies","text":"<pre><code>from transformers import AutoImageProcessor, ViTModel\nimport torch\nimport torch.nn as nn\nfrom baseline.vision_transformer import VisionTransformer\nfrom einops import rearrange\n</code></pre>"},{"location":"TemporalVisionTransformer/#class-temporalvisiontransformer","title":"Class: <code>TemporalVisionTransformer</code>","text":"<pre><code>class TemporalVisionTransformer(nn.Module):\n</code></pre>"},{"location":"TemporalVisionTransformer/#description","title":"Description","text":"<p>The <code>TemporalVisionTransformer</code> class is a neural network model for temporal image sequence analysis, inheriting from <code>nn.Module</code>. It integrates a Vision Transformer (ViT) adapted for temporal data processing, with positional encoding for the temporal dimension to encode time-related information.</p>"},{"location":"TemporalVisionTransformer/#parameters","title":"Parameters","text":"<ul> <li><code>img_size</code> (int): Size of the input image (assumed square). Default is 128.</li> <li><code>patch_size</code> (int): Size of patches within the image. Default is 16.</li> <li><code>in_channels</code> (int): Number of input channels per image in the sequence. Default is 10.</li> <li><code>seq_length</code> (int): Number of images in the sequence. Default is 61.</li> <li><code>embed_dim</code> (int): Embedding dimension for transformer. Default is 768.</li> <li><code>num_heads</code> (int): Number of attention heads in each transformer layer. Default is 5.</li> <li><code>num_layers</code> (int): Number of layers in the transformer. Default is 6.</li> <li><code>num_classes</code> (int): Number of output classes. Default is 1.</li> </ul>"},{"location":"TemporalVisionTransformer/#attributes","title":"Attributes","text":"<ul> <li><code>vit</code> (<code>VisionTransformer</code>): The core Vision Transformer module for extracting features from each image.<ul> <li>Configured with <code>img_size</code>, <code>patch_size</code>, <code>num_layers</code>, <code>num_heads</code>, <code>in_channels</code>, <code>embed_dim</code>, and <code>num_classes</code>.</li> </ul> </li> <li><code>seq_length</code> (int): Length of the temporal sequence.</li> <li><code>embed_dim</code> (int): Dimension for embedding in the temporal context.</li> <li><code>temporal_positional_encoding</code> (<code>nn.Parameter</code>): Positional encoding for each frame in the sequence, enhancing temporal awareness in the transformer model.</li> <li><code>decoder</code> (<code>nn.Sequential</code>): Decoder to upsample the features and output a segmentation map</li> </ul>"},{"location":"TemporalVisionTransformer/#methods","title":"Methods","text":"<ul> <li><code>forward(x: torch.Tensor) -&gt; torch.Tensor</code><ul> <li>Forward pass for the temporal Vision Transformer.</li> <li>Parameters:<ul> <li><code>x</code> (<code>torch.Tensor</code>): Input tensor of shape <code>(batch_size, seq_length, in_channels, img_size, img_size)</code>.</li> </ul> </li> <li>Returns:<ul> <li><code>torch.Tensor</code>: Output tensor for classification or regression tasks, typically of shape <code>(batch_size, num_classes)</code>.</li> </ul> </li> </ul> </li> </ul>"},{"location":"TemporalVisionTransformer/#example-usage","title":"Example Usage","text":"<pre><code>model = TemporalVisionTransformer(img_size=128, patch_size=16, in_channels=10, seq_length=61, embed_dim=768)\ninput_tensor = torch.randn(1, 61, 10, 128, 128)  # Example input\noutput = model(input_tensor)\nprint(output.shape)  # Expected output shape depends on num_classes (default is (1, 1))\n</code></pre>"},{"location":"TemporalVisionTransformer/#notes","title":"Notes","text":"<ul> <li>This model includes a ViT backbone for spatial feature extraction, with adjustments for handling sequences in the temporal dimension.</li> <li>Temporal positional encoding would allows the model to capture dependencies across the sequence length, which may improve performance on tasks involving sequential data.</li> </ul>"},{"location":"YOLO/","title":"1. YOLO implementation","text":"<p>You can find it in the <code>yolo/</code> folder. We tried to run YOLO on our data but quickly stopped due to YOLO being trained with polygonal bounding boxes for segmentation. Since our data is only annotated with the masks of the classes on our images, it is not possible to directly feed these annotations to YOLO.</p> <p>While trying to implement this code, we chose to give up on the temporal dimension, selecting only one image per sequence and choosing only the RBG channels in order to feed the images to YOLO.</p>"},{"location":"prithvi/","title":"2. Implementation of a ViT pretrained on a crop segmentation task","text":"<p>You can find this implementation in <code>prithvi_notebook.ipynb</code>. Here, we used a promising network already pretrained and adaptated to our task, yet we weren't able to retrieve all the code, so we had to recode part of the network. It resulted in a working implementation that takes too long to train and gives poor results on the few epochs on which it was trained.</p>"}]}